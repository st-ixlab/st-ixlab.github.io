@inproceedings{Min2025Silent,
  venue_type = {international},
  preview = {Min2025Silent.png},
  abbr = {Top-Tier Conference},
  title = {Silent Yet Expressive: Toward Seamless VR Communication through Emotion-aware Silent Speech Interfaces},
  author = {Yewon Min and Jiyeon Oh and Jae-Yeop Jeong and Jin-Woo Jeong},
  year = {2025},
  booktitle = {ACM Symposium on User Interface Software and Technology (UIST) Posters},
  location = {Busan, Korea},
  month = {Sep},
  doi = {10.1145/3746058.3758359},
  abstract = {The paper addresses the fundamental challenge of building affective intelligent systems that are aware of the emotions of those communicating. It highlights that emotion is a complex mental state influenced by external events, physiological changes, or relationships. The core aim of the research is the development of an unobtrusive emotion recognition approach and a user scenario for emotion-oriented social communication in VR, enabling users to communicate silently with computing devices without audible speech or discernible movements.}
}
@inproceedings{Min2025Exploring,
  venue_type = {international},
    preview = {Min2025Exploring.png},
  abbr = {Top-Tier Conference},
  title = {Exploring Emotion Expression Through Silent Speech Interface in Public VR/MR: Effects of Automation on User Experience},
  author = {Yewon Min and Jiyeon Oh and Jae-Yeop Jeong and Jin-Woo Jeong},
  year = {2025},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems (ACM CHI) Late-Breaking Works},
  location = {Yokohama, Japan},
  month = {May},
  doi = {10.1145/3706599.3720209},
  abstract = {Silent speech input offers a secure and private alternative to voice input in public spaces. As HMDs become increasingly mobile, the need for integrating silent speech recognition grows to enable seamless interaction in public settings. However, the potential of silent speech-based emotion recognition in VR/MR environments remains underexplored, despite the importance of emotional expression in social interactions. This paper evaluates how different levels of automation in emotional expression—Manual, Semi-auto, and Automatic—affect user experience in VR environments. The results revealed that Semi-auto methods generally offer better efficiency, user control, and socially acceptable interactions.}
}
@inproceedings{Jeong2025Understanding,
  venue_type = {international},
    preview = {Jeong2025Understanding.png},
  abbr = {Top-Tier Conference},
  title = {Understanding User Behavior in Window Selection using Dragging for Multiple Targets},
  author = {Jae-Yeop Jeong and Jin-Woo Jeong},
  year = {2025},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems (ACM CHI)},
  location = {Yokohama, Japan},
  month = {May},
  doi = {10.1145/3706598.3713410},
  abstract = {Window selection is a fundamental part of graphical user interfaces (GUIs). Although the dragging technique is widely used for selecting multiple items, its application in window selection has not been thoroughly studied. This paper presents an experimental study investigating user behavior in window selection using dragging for multiple targets. We analyze the performance and preferences of users, providing insights into improving the design of window selection interactions. Our findings highlight the effectiveness of dragging for selecting multiple windows and suggest design guidelines for enhancing user experience in multi-window environments.},  
  award={Honorable Mention},
  award_name = {Honorable Mention}
}
@inproceedings{Kim2025Through,
  venue_type = {international},
    preview = {Kim2025Through.png},
  abbr = {Top-Tier Conference},
  title = {Through the Looking Glass, and What We Found There: A Comprehensive Study of User Experiences with Pass-Through Devices in Everyday Activities},
  author = {Daewook Kim and Yewon Min and Jae-Yeop Jeong and Sehee Han and JiYeon Hwang and Jin-Woo Jeong},
  year = {2025},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems (ACM CHI)},
  location = {Yokohama, Japan},
  month = {May},
  doi = {10.1145/3706598.3714221},
  abstract = {Pass-through technologies are promising for mixed reality (MR) systems. Therefore, various MR applications operating in pass-through devices emerged in diverse domains, such as education and healthcare. However, research on the everyday use of pass-through devices remains limited, despite it blending real and virtual environments. This study explores the user experience of pass-through devices in people’s daily tasks. We conducted a field study with 16 participants and analyzed data from eight daily tasks. For in-depth analysis, we employed three measures in terms of quantitative, qualitative, and bio-signal. As a result, we found that participants felt differently in terms of immersion, collision anxiety, and workload. Findings suggest that pass-through devices are not yet fully ready for integration into daily life. However, the potential for widespread adoption exists as the technology continues to advance. Finally, we offer guidelines and considerations to improve the usability of pass-through devices for everyday use.}
  
  }
@inproceedings{Lukianova2025Picture,
  venue_type = {international},
    preview = {Lukianova2025Picture.png},
  abbr = {Top-Tier Conference},
  title = {A picture is worth a thousand words? Investigating the Impact of Image Aids in AR on Memory Recall for Everyday Tasks},
  author = {Elizaveta Lukianova and Jae-Yeop Jeong and Jin-Woo Jeong},
  year = {2025},
  booktitle = {ACM Conference on Intelligent User Interfaces (ACM IUI)},
  location = {Cagliari, Italy},
  month = {Mar},
  doi = {10.1145/3708359.3712087},
  abstract = {This paper investigates the impact of image aids in AR on memory recall for everyday tasks. It explores how Augmented Reality (AR) can enhance learning by supporting embodied interaction, contextual immersion, and multimodal engagement. The study suggests that image aids in AR can significantly improve memory retention and facilitate the comprehension of abstract concepts.},
  award={Best Paper},
  award_name = {Best Paper}
}
@article{Kim2025Systematic,
  venue_type = {korean},
  title = {A Systematic Literature Review on Personality Modeling and Expression for Social Robot Personalization Services},
  author = {Shee-Ihn Kim and Jin-Woo Jeong},
  year = {2025},
  journal = {Journal of the Korea Academia-Industrial cooperation Society},
  note = {Accepted}
}
@inproceedings{Han2025Exploring,
  venue_type = {korean},
  title = {Exploring the Effects of Immediate and Gradual UX Transitions during Short-form Video Viewing on Time Perception and Emotional Response},
  author = {Sehee Han and Jiyeon Oh and Yewon Min and Jin-Woo Jeong},
  year = {2025},
  booktitle = {HCI Korea 2025}
}
@inproceedings{Hwang2025Personal,
  venue_type = {korean},
  title = {Personal Baseball: LLM-based User-Customized Baseball Highlight Video Generation System},
  author = {Jiyeon Hwang and Jiyeon Oh and Dae Wook Kim and Jin-Woo Jeong},
  year = {2025},
  booktitle = {HCI Korea 2025}
}
@article{Choi2024Eeg,
  venue_type = {international},
    preview = {Choi2024Eeg.png},
  title = {EEG Dataset for the Recognition of Different Emotions Induced in Voice-User Interaction},
  author = {Ga-Young Choi and Jong-Gyu Shin and Ji-Yoon Lee and Jun-Seok Lee and In-Seok Heo and Ha-Yeong Yoon and Wansu Lim and Jin-Woo Jeong and Sang-Ho Kim and Han-Jeong Hwang},
  year = {2024},
  journal = {Scientific Data},
  volume = {11},
  number = {1084},
  doi = {https://doi.org/10.1038/s41597-024-03887-9},
  abstract = {Electroencephalography (EEG)-based open-access datasets are available for emotion recognition studies, typically using external auditory or visual stimuli to artificially evoke predefined emotions. This study introduces a novel EEG dataset that captures emotional information induced during a realistic human-computer interaction (HCI) with a voice user interface system, designed to mimic natural human-to-human communication. To validate this dataset, researchers applied a series of signal processing and machine learning methods to the EEG data for neurophysiological investigation and binary emotion classification. The maximum classification accuracy achieved ranged from 43.3% to 90.8% across 38 subjects, and the classification features could be interpreted neurophysiologically. This EEG data has the potential to contribute to the development of reliable HCI systems.}
}
@article{Oh2024Mitigating,
  venue_type = {international},
    preview = {Oh2024Mitigating.png},
  title = {Mitigating Inappropriate Concepts in Text-to-Image Generation with Attention-guided Image Editing},
  author = {Jiyeon Oh and Jae-Yeop Jeong and Yeong-Gi Hong and Jin-Woo Jeong},
  year = {2024},
  journal = {PeerJ Computer Science},
  note = {Accepted},
  doi = {10.7717/peerj-cs.3170},
  abstract = {Text-to-image generative models have recently garnered a significant surge due to their ability to produce diverse images based on given text prompts. However, concerns regarding the occasional generation of inappropriate, offensive, or explicit content have arisen. To address this, a simple yet effective method is proposed that leverages attention maps to selectively suppress inappropriate concepts during image generation. Unlike existing approaches that often sacrifice original image context or demand substantial computational overhead, this method preserves image integrity without requiring additional model training or extensive engineering effort. To evaluate the method, comprehensive quantitative assessments were conducted on inappropriateness reduction, text fidelity, image consistency, and computational cost, alongside an online human perceptual study involving 20 participants. The results from the statistical analysis demonstrated that the method effectively removes inappropriate content while preserving the integrity of the original images with high computational efficiency.}
}
@unpublished{TBD2024Enhancing,
  venue_type = {international},
  title = {Enhancing Multitasking in Mixed Reality: Design and Assessment of Visual Aids for Managing Interruptions},
  author = {TBD},  
  note = {Under review}
}
@inproceedings{Kim2024DesigningLLM,
preview = {Kim2024DesigningLLM.png},
  venue_type = {international},  
  title = {Designing LLM Response Layouts for XR Workspaces in Vehicles},
  author = {Daun Kim and Jin-Woo Jeong},
  year = {2024},
  booktitle = {ACM SIGGRAPH ASIA},
  location = {Tokyo, Japan},
  month = {Dec},
  doi = {10.1145/3681756.3697877},
  abstract = {This study investigates how large language model (LLM) responses can be effectively presented in Extended Reality (XR) environments within vehicles. The research highlights usability challenges associated with current linear layouts in XR, such as difficulties with extensive scrolling and mid-air interactions. To address these issues, the authors propose an improved layout design that utilizes multiple windows for efficient access to diverse content. This new layout also features a cohesive placement strategy aimed at minimizing motion sickness. A user study involving 24 participants was conducted to compare this proposed interface with a conventional web browser interface across various information-seeking tasks. The paper aims to inform the design of more effective and comfortable LLM response layouts for in-vehicle XR workspaces.}
  }
@inproceedings{Min2024Public,
  venue_type = {international},
    preview = {Min2024Public.png},
  abbr = {Top-Tier Conference},
  title = {Public Speaking Q&A Practice with LLM-Generated Personas in Virtual Reality},
  author = {Yewon Min and Jin-Woo Jeong},
  year = {2024},
  booktitle = {IEEE International Symposium on Mixed and Augmented Reality (ISMAR '24)},
  location = {Seattle, USA},
  month = {Oct},
  doi = {10.1109/ISMAR-Adjunct64951.2024.00143},
  abstract = {This paper introduces a novel Virtual Reality (VR)-based Q&A practice system that leverages the power of Large Language Models (LLMs). The system aims to support Q&A practice for upcoming public speaking engagements by offering an immersive VR training environment. This environment is populated with LLM-generated audiences, each designed to pose diverse and realistic questions based on distinct personas. A pilot user study was conducted with 20 participants who engaged in these VR-based Q&A practice sessions. During these sessions, participants encountered a variety of questions related to their provided presentation material, all of which were generated by the LLM-based personas. Through post-surveys and interviews, the study evaluated the effectiveness of this proposed method. Participants valued the system for its engagement and ability to foster focus, though they also identified areas for improvement. The study ultimately demonstrated the potential of integrating VR and LLMs to create a powerful and immersive tool for Q&A practice.}
}
@inproceedings{Kim2024DesigningResponse,
  venue_type = {international},
    preview = {Kim2024DesigningResponse.png},
  title = {Designing Response Layouts of LLM-based Conversational Agents for Extended Reality Environments in Vehicles},
  author = {Daun Kim and Jin-Woo Jeong},
  year = {2024},
  booktitle = {Technical Congress of the International Ergonomics Association (IEA 24)}
}
@inproceedings{Jeong2024Effect,
  venue_type = {international},
    preview = {Jeong2024Effect.png},
  abbr = {Top-Tier Conference},
  title = {Effect of Onset Position of Ray Casting in Virtual Reality},
  author = {Jae-Yeop Jeong and Jin-Woo Jeong},
  year = {2024},
  booktitle = {ACM SIGCHI Conference on Human Factors in Computing Systems (CHI'24) Late-Breaking Works},
  location = {Hawaiʻ, USA},
  month = {May},
  doi = {10.1145/3613905.3650905},
  abstract = {In virtual reality (VR), interaction techniques significantly influence user experience and performance. This study explores the effect of the onset position of ray casting in VR through a target selection task. Researchers designed a pointing task (specifically, a Fitts' law task) to collect data with various onset positions. The data was then analyzed to determine how different onset positions affect individual performance for 3D interaction in terms of pointing and selection. The results indicated significant effects for each participant, but no generalized impact was found across all participants. These findings highlight the complexity of human-computer interaction in ray casting and suggest that a one-size-fits-all approach may not be effective. For future research and practical applications, the study advocates a personalized approach that considers the specific preferences and responses of individual users. This tailored strategy could potentially improve the effectiveness and usability of interactive systems that utilize ray cast techniques.}  
}

@inproceedings{Kim2024Development,
  venue_type = {korean},
  title = {Development of Deep Learning-based Real-time Learner State Recognition and Visualization System for Enhancing Immersive Content Learning Experience},
  author = {Daun Kim and Ha-Neul Kim and Yewon Min and Se-Yeon Oh and Chae-Rin Lee and Yujin Lee and Jeong-Won Lee and Jin-Woo Jeong},
  year = {2024},
  booktitle = {HCI Korea 2024}
}
@inproceedings{Lukianova2024Study,
  venue_type = {korean},
  title = {Study on the Feedback Modes of Memory Augmentation System for Everyday Life},
  author = {Lukianova Elizaveta and Jin-Woo Jeong},
  year = {2024},
  booktitle = {한국통신학회 인공지능학술대회 2024}
}
@inproceedings{Oh2024Attention,
  venue_type = {korean},
  title = {Attention Map-based Technique Proposal and User Evaluation for Reducing Inappropriateness in Text-to-Image Generation},
  author = {Jiyeon Oh and Jae-Yeop Jeong and Yeong-Gi Hong and Jin-Woo Jeong},
  year = {2024},
  booktitle = {대한인간공학회 2024 추계}
}
@inproceedings{Lukianova2024Preliminary,
  venue_type = {korean},
  title = {A Preliminary Study on Real-time Memory Augmentation via Image and Text-based Feedback for Everyday Tasks using Multimodal Large Language Models},
  author = {Lukianova Elizaveta and Jin-Woo Jeong},
  year = {2024},
  booktitle = {대한인간공학회 2024 추계}
}
@article{Jeong2023EchoTap,
  venue_type = {international},
    preview = {Jeong2023EchoTap.png},
    abbr = {Top-Tier Journal},
  title = {EchoTap: Non-verbal Sound Interaction with Knock and Tap Gestures},
  author = {Jae-Yeop Jeong and Daun Kim and Jin-Woo Jeong},
  year = {2023},
  journal = {International Journal of Human-Computer Interaction},
  note = {Accepted},
  doi = {10.1080/10447318.2024.2348837},
  abstract = {The growing demand for highly accessible interaction technologies to effectively interact with smart devices has led to the increasing popularity of voice user interfaces (VUIs). However, VUIs face interpretation challenges stemming from the variability of natural language input, such as speech clarity issues, linguistic variability, and speech impediments. As an alternative, non-verbal sound-based interaction techniques emerge as highly advantageous for smart device control, mitigating the inherent challenges of VUIs. This article introduces EchoTap, a novel audio interface that harnesses the distinctive sound responses generated by knock and tap gestures on target objects. Employing deep neural networks, EchoTap recognizes both the type and location of these gestures based on their unique sound signatures. Through offline evaluation, EchoTap demonstrated competitive classification accuracy (88% on average) and localization precision (93% on average). Moreover, a user study involving 12 participants validated EchoTap's practical effectiveness and user-friendliness in real-world scenarios. This study highlights EchoTap's potential for various daily interaction contexts and discusses further design implications for leveraging auditory interfaces based on simple gestures.}
}
@article{Jeong2023Enhancing,
  venue_type = {international},
    preview = {Jeong2023Enhancing.png},
    abbr = {Top-Tier Journal},
  title = {Enhancing Learner Experience with Instructor Cues in Video Lectures: A Comprehensive Exploration and Design Discovery toward A Novel Gaze Visualization},
  author = {Jae-Yeop Jeong and JiYeon Oh and Jin-Woo Jeong},
  year = {2023},
  journal = {Education and Information Technologies},
  note = {Accepted},
  doi = {10.1007/s10639-024-12697-w}
}
@unpublished{TBD2023MuNext,
  venue_type = {international},
  title = {MuNext: Multi-task UNeXt for Efficient Breast Cancer Classification and Segmentation},
  author = {TBD},  
  note = {Under review}
}
@unpublished{TBD2023Classification,
  venue_type = {international},  
  title = {Classification of Breast Cancer from Mammograms: Do Vision Transformers and MLP-Mixers Outperform CNNs?},
  author = {TBD},  
  note = {Under review}
}
@inproceedings{Kim2023EmoFlow,
  venue_type = {international},
    preview = {Kim2023EmoFlow.png},
  title = {EmoFlow: Visualizing Emotional Changes in Video Chat – Preliminary Study},
  author = {Daun Kim and Ji Yeon Oh and Jae-Yeop Jeong and Jin-Woo Jeong},
  year = {2023},
  booktitle = {ACM International Conference on Mobile Human Computer Interaction (MobileHCI) Late-Breaking Works},
  location = {Athens, Greece},
  month = {Sep},
  doi = {10.1145/3565066.3608702},
  abstract = {We propose EmoFlow, a prototype service that can recognize the emotional changes of users in video chat and provide a text-based summary along with various visualizations. We present design concepts to represent users' emotional flow on the mobile messenger application in three types: Emoji, Color, and Shape visualizations. Through a user study with 60 voluntary participants, we found that the Emoji type received the highest score in terms of conveying emotions and design preferences. Also, 65 percent of the participants responded they are willing to use the proposed service if several points are enhanced in the future. The survey results suggest that the proposed service would help users understand and manage their emotional status.}  
}
@inproceedings{Hong2023Dynamic,
  venue_type = {international},
    preview = {Hong2023Dynamic.png},
  title = {Dynamic Noise Injection for Facial Expression Recognition In-the-Wild},
  author = {SangHwa Hong and Jin-Woo Jeong},
  year = {2023},
  booktitle = {IEEE/CVF CVPR workshop on Affective Behavior Analysis in-the-wild},
  location = {Vancouver, Canada},
  month = {Jun},
  abstract = {Facial expression-based emotion analysis is a crucial area of research in artificial intelligence. However, many current works are hindered by low classification or regression performance due to overfitting. To address this, the paper introduces a novel noise injection technique. Specifically, this method, built upon the ResNet-18 architecture, dynamically injects feature-level noise into the BN+ReLU unit to facilitate the learning of more robust features. Experiments conducted on facial expression classification using the AffectNet dataset have demonstrated the effectiveness of this proposed approach. The paper also highlights that, unlike traditional noise injection techniques where noise is continuously introduced, their approach proposes dynamically injecting noise during the training process for selected epochs.}
}

@INPROCEEDINGS{11236281,
  venue_type = {international},
    preview = {11236281.png},
  abbr = {Top-Tier Conference},
  author={Oh, Jiyeon and Jeong, Jin-Woo},
  booktitle={2025 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)}, 
  title={Exploring Interface Design of Translation System for Enhanced Immersion and Usability in Mixed Reality}, 
  year={2025},
  volume={},
  number={},
  pages={719-720},
  keywords={Visualization;Translation;Text recognition;User centered design;Mixed reality;Integrated design;Rendering (computer graphics);User experience;Real-time systems;Usability;Translation system;Mixed Reality;Pass-through},
  doi={10.1109/ISMAR-Adjunct68609.2025.00156}
}

@article{OH2026103673,
venue_type = {international},
preview = {OH2026103673.png},
abbr = {Top-Tier Journal},
title = {SnapSound: Empowering everyone to customize sound experience with Generative AI},
journal = {International Journal of Human-Computer Studies},
volume = {207},
pages = {103673},
year = {2026},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2025.103673},
url = {https://www.sciencedirect.com/science/article/pii/S1071581925002307},
author = {Jiyeon Oh and Jin-Woo Jeong},
keywords = {Sound design, Generative AI, Assistive system, User-centered design},
abstract = {The rise of user-centric experiences in the digital landscape has led to a surge in demand for personalized multimedia content. Users now seek to customize not only visual but also auditory components to suit their preferences. In this context, sound design plays a crucial role, enabling users to tailor audio experiences accordingly. However, its inherent complexity poses various challenges, particularly for non-expert users. To address this challenge, we introduce SnapSound, a novel assistive system designed specifically for non-experts in sound design for video content. Our system leverages generative AI to streamline the sound design process and offers intuitive tools for sound selection, synchronization, and seamless integration with visuals. Through a user study, we evaluate SnapSound’s usability and effectiveness compared to manual editing. Furthermore, our study provides valuable insights and design recommendations for enhancing user experience of future AI-based sound design systems. This work represents a significant step forward in empowering non-experts to easily customize their auditory experiences.}
}

@inproceedings{Oh2023Tingle,
  venue_type = {international},
    preview = {Oh2023Tingle.png},
  abbr = {Top-Tier Conference},
  title = {Tingle Just for You: A Preliminary Study of AI-based Customized ASMR Content Generation},
  author = {Ji Yeon Oh and Daun Kim and Jae-Yeop Jeong and Elizaveta Lukianova and Jin-Woo Jeong},
  year = {2023},
  booktitle = {ACM SIGCHI Conference on Human Factors in Computing Systems (CHI'23) Late-Breaking Works},
  location = {Germany},
  month = {Apr},
  doi ={10.1145/3544549.3585872},
  abstract = {This paper discusses research into Autonomous Sensory Meridian Response (ASMR) and the development of AI to generate personalized ASMR content. It explores how ASMR, characterized by pleasant tingling sensations and feelings of relaxation, can be customized using AI. It notes that ASMR videos are not just about tingling, but also offer experiences of social connection, physical intimacy, and activity observation. The study emphasizes the potential for designing video-based applications that incorporate ASMR effects.}
  }
@inproceedings{Hong2023Spot,
  venue_type = {international},
    preview = {Hong2023Spot.png},
  abbr = {Top-Tier Conference},
  title = {Spot The Difference: AI, Please Make This for Me!},
  author = {Yeong-Gi Hong and Jae-Yeop Jeong and Elizaveta Lukianova and Jin-Woo Jeong},
  year = {2023},
  booktitle = {ACM SIGCHI Conference on Human Factors in Computing Systems (CHI'23) Late-Breaking Works},
  location = {Germany},
  month = {Apr},
  doi = {10.1145/3544549.3585879},
  abstract = { This paper introduces a novel AI-driven framework that streamlines the generation of "spot the difference" challenges. By leveraging advanced generative AI models, such as text-to-image systems and image-to-image modification tools, the system can create a base image from a user-defined prompt and subsequently introduce specific, controlled alterations to generate a second, divergent image. This methodology significantly reduces the need for human intervention in content creation, enabling the production of a vast and diverse array of puzzles. Furthermore, the framework incorporates computer vision techniques to identify and manage these differences, ensuring the integrity and playability of the generated games.}
}
}
@article{Hong2023Automatic,
  venue_type = {international},
    preview = {Hong2023Automatic.png},
  title = {Automatic Examination of Condition of Used Books with YOLO-based Object Detection Framework},
  author = {Sumin Hong and Jin-Woo Jeong},
  year = {2023},
  journal = {Computer Systems Science and Engineering},
  volume = {47},
  number = {2},
  pages = {1611-1632},
  doi = {10.32604/csse.2023.038319}
}
@unpublished{TBD2023Efficient,
  venue_type = {international},  
  title = {Efficient Multi-Task Learning for Facial Expression Recognition In-the-wild},
  author = {TBD},  
  note = {Under review}
}
@inproceedings{Yoon2023Deep,
  venue_type = {korean},
  title = {Deep Learning-based Event Recognition Technology in Mobile 360 Video for Expanding Social Safety Net},
  author = {Ha-Yeong Yoon and Daun Kim and Jiyeon Oh and Sumin Hong and Jiheun Seo and Jin-Woo Jeong},
  year = {2023},
  booktitle = {HCI Korea 2023}
}
@inproceedings{Yoon2023Comparative,
  venue_type = {korean},
  title = {Comparative Experiment of Breast Cancer Classification for Deep Learning-based Models Considering Mobile Environments},
  author = {Ha-Yeong Yoon and Seun Choi and Jin-Woo Jeong},
  year = {2023},
  booktitle = {HCI Korea 2023}
}
@inproceedings{Jeong2023Deep,
  venue_type = {korean},
  title = {Deep Learning-based User Emotional Satisfaction Evaluation Model Construction and Pruning Method for User-Responsive Conversational Agents},
  author = {Jae-Yeop Jeong and Yeong-Gi Hong and Wansu Lim and Jin-Woo Jeong},
  year = {2023},
  booktitle = {HCI Korea 2023}
}
@inproceedings{Hong2023Deep,
  venue_type = {korean},
  title = {Deep Learning Model for Detecting Risky Behavior of Children in Vehicles},
  author = {Sang-Hwa Hong and Jin-Woo Jeong},
  year = {2023},
  booktitle = {HCI Korea 2023}
}
@inproceedings{Kim2023Understanding,
  venue_type = {korean},
  title = {Understanding User's Spatial Context Using Background Noise in Voice Call Environment},
  author = {Doyoung Kim and Daun Kim and Jin-Woo Jeong},
  year = {2023},
  booktitle = {KCC 2023},
  award_name = {Outstanding Paper},
  award = {Outstanding Paper}  
}
@inproceedings{Kim2023Development,
  venue_type = {korean},
  title = {Development of Deep Audio Transfer Learning-based Gesture Classification and Localization Model for Interaction with Immersive Virtual Reality},
  author = {Namsub Kim and Jae-Yeop Jeong and Jin-Woo Jeong},
  year = {2023},
  booktitle = {KCC 2023}
}
@inproceedings{Min2023Transportation,
  venue_type = {korean},
  title = {Transportation Vulnerable Detection System for Passenger Safety in Autonomous Buses},
  author = {Yewon Min and Jiyeon Oh and Jin-Woo Jeong},
  year = {2023},
  booktitle = {KCC 2023},
  award_name = {Outstanding Paper}  
}
@inproceedings{Hong2023Estimation,
  venue_type = {korean},
  title = {Estimation of Appropriateness of CPR Compression Frequency and Depth based on HoloLens 2},
  author = {Sang-Hwa Hong and Jin-Woo Jeong},
  year = {2023},
  booktitle = {KCC 2023},
  award_name = {Outstanding Paper},
  award = {Outstanding Paper}
}
@article{Pascual2022Light,
  venue_type = {international},
    preview = {Pascual2022Light.png},
  title = {Light-FER: A Lightweight Facial Emotion Recognition System on Edge Devices},
  author = {Alexander M Pascual and Erick C Valverde and Jeong-In Kim and Jin-Woo Jeong and Yuchul Jung and Sang-Ho Kim and Wansu Lim},
  year = {2022},
  journal = {Sensors},
  volume = {22},
  number = {23},
  pages = {9524},
  doi = {10.3390/s22239524},
  abstract = {Facial emotion recognition (FER) systems are crucial in advanced artificial intelligence (AI) applications for enhanced human-computer interactions. Most deep learning-based FER systems face challenges due to low accuracy and high resource demands, particularly when implemented on edge devices with limited computing power and memory. To address these issues, this paper introduces Light-FER, a lightweight FER system derived from the Xception model through model compression techniques. The process involves several steps: First, pruning is applied during network training to eliminate less significant connections within the Xception architecture. Second, the model is quantized to a half-precision format, substantially reducing its memory footprint. Third, various deep learning compilers are benchmarked to identify and utilize advanced optimization techniques for accelerating the FER system's inference speed. Finally, to experimentally validate the proposed system's objectives on edge devices, Light-FER is deployed on an NVIDIA Jetson Nano.}
}
@article{Mudeng2022Simply,
  venue_type = {international},
    preview = {Mudeng2022Simply.png},
  title = {Simply Fine-Tuned Deep Learning-Based Classification for Breast Cancer with Mammograms},
  author = {Mudeng V and Jin-Woo Jeong and Se-woon Choe},
  year = {2022},
  journal = {CMC-Computers, Materials & Continua},
  volume = {73},
  number = {3},
  doi = {10.32604/cmc.2022.031046}
}
@article{Ayana2022Novel,
  venue_type = {international},
    preview = {Ayana2022Novel.png},
  title = {A Novel Multistage Transfer Learning for Ultrasound Breast Cancer Image Classification},
  author = {Gelan Ayana and Jinhyung Park and Jin-Woo Jeong and Se-woon Choe},
  year = {2022},
  journal = {Diagnostics},
  volume = {12},
  number = {1},
  pages = {135},
  doi = {10.3390/diagnostics12010135},
  abstract = {Breast cancer diagnosis is one of the many areas that has taken advantage of artificial intelligence to achieve better performance, despite the fact that the availability of a large medical image dataset remains a challenge. Transfer learning (TL) is a phenomenon that enables deep learning algorithms to overcome the issue of shortage of training data in constructing an efficient model by transferring knowledge from a given source task to a target task. However, in most cases, ImageNet (natural images) pre-trained models, which do not include medical images, are utilized for transfer learning to medical images. Considering the utilization of microscopic cancer cell line images that can be acquired in large amounts, the authors argue that learning from both natural and medical datasets improves performance in ultrasound breast cancer image classification. The proposed multistage transfer learning (MSTL) algorithm was implemented using three pre-trained models: EfficientNetB2, InceptionV3, and ResNet50 with three optimizers: Adam, Adagrad, and stochastic gradient descent (SGD). Dataset sizes of 20,400 cancer cell images, 200 ultrasound images from Mendeley, and 400 ultrasound images from the MT-Small-Dataset were used. ResNet50-Adagrad-based MSTL achieved a test accuracy of 99 ± 0.612% on the Mendeley dataset and 98.7 ± 1.1% on the MT-Small-Dataset, averaging over 5-fold cross-validation. A p-value of 0.01191 was achieved when comparing MSTL against ImageNet based TL for the Mendeley dataset. The result is a significant improvement in the performance of artificial intelligence methods for ultrasound breast cancer classification compared to state-of-the-art methods and could remarkably improve the early diagnosis of breast cancer in young women.}
}
@article{Choe2022Automatic,
  venue_type = {international},
    preview = {Choe2022Automatic.png},
  title = {Automatic cancer cell taxonomy using an ensemble of deep neural networks},
  author = {Se-woon Choe and Ha-Yeong Yoon and Jae-Yeop Jeong and Jinhyung Park and Jin-Woo Jeong},
  year = {2022},
  journal = {Cancers},
  volume = {14},
  number = {9},
  pages = {2224},
  doi = {10.3390/cancers14092224}
}
@inproceedings{Jeong2022Ensemble,
  venue_type = {international},
    preview = {Jeong2022Ensemble.png},
  title = {Ensemble of Multi-task Learning Networks for Facial Expression Recognition In-the-Wild with Learning from Synthetic Data},
  author = {Jae-Yeop Jeong and Yeong-Gi Hong and Sumin Hong and JiYeon Oh and Yuchul Jung and Sang-Ho Kim and Jin-Woo Jeong},
  year = {2022},
  booktitle = {IEEE/CVF ECCV workshop on Affective Behavior Analysis in-the-wild},
  location = {Israel},
  month = {Oct},
  doi = {10.1007/978-3-031-25075-0_5},
  abstract = {This paper presents a computationally efficient ensemble of multi-task learning networks for facial expression recognition in-the-wild. The approach addresses the challenges of variability in head poses, occlusions, and illumination conditions often encountered in real-world scenarios. The proposed method utilizes an ensemble of networks, each trained on different tasks related to facial analysis, to improve the robustness and accuracy of facial expression recognition. Furthermore, an analysis of space and time complexity was conducted.}
}
@inproceedings{Yoon2022Classification,
  venue_type = {international},
    preview = {Yoon2022Classification.png},
  title = {Classification of Breast Cancer Images with Vision Transformers},
  author = {Ha-Yeong Yoon and Se-woon Choe and Jin-Woo Jeong},
  year = {2022},
  booktitle = {IEEE International Engineering in Medicine and Biology Conference (EMBC)},
  location = {Glasgow, UK},
  month = {Jul}
}
@inproceedings{Jeong2022Classification,
  venue_type = {international},
    preview = {Jeong2022Classification.png},
  title = {Classification of Facial Expression In-the-Wild based on Ensemble of Multi-head Cross Attention Networks},
  author = {Jae-Yeop Jeong and Yeong-Gi Hong and Daun Kim and Yuchul Jung and Sang-Ho Kim and Jin-Woo Jeong},
  year = {2022},
  booktitle = {IEEE/CVF CVPR workshop on Affective Behavior Analysis in-the-wild},
  location = {New Orleans, US},
  month = {Jun}
}
@inproceedings{Kim2022Visualizing,
  venue_type = {international},
    preview = {Kim2022Visualizing.png},
  title = {Visualizing Instructor's Gaze Information for Online Video-based Learning: Preliminary Study},
  author = {Daun Kim and Jae-Yeop Jeong and Sumin Hong and Namsub Kim and Jin-Woo Jeong},
  year = {2022},
  booktitle = {ACM ETRA workshop on Eye Tracking in Learning and Education},
  location = {Seattle, US},
  month = {Jun},
  doi = {10.1145/3517031.3529238}
}
@article{Yoon2022Classification,
  venue_type = {international},
    preview = {Yoon2022Classification.png},
  title = {Classification of the Sidewalk Condition Using Self-Supervised Transfer Learning for Wheelchair Safety Driving},
  author = {Ha-Yeong Yoon and Jung-Hwa Kim and Jin-Woo Jeong},
  year = {2022},
  journal = {Sensors},
  volume = {22},
  number = {1},
  pages = {380}
}
@article{Kim2022Multi,
  venue_type = {international},
    preview = {Kim2022Multi.png},
  title = {Multi-view Multi-modal Head-Gaze Estimation for Advanced Indoor User Interaction},
  author = {Jung-Hwa Kim and Jin-Woo Jeong},
  year = {2022},
  journal = {Computers, Materials & Continua},
  volume = {70},
  number = {3},
  pages = {5107-5132}
}
@article{Njoku2022Deep,
  venue_type = {korean},
  title = {Deep Learning Based Data Fusion Methods for Multimodal Emotion Recognition},
  author = {Judith Nkechinyere Njokuw and Angela C. Caliwag and Wansu Lim and Sangho Kim and Han-Jeong Hwang and Jin-Woo Jeong},
  year = {2022},
  journal = {The Journal of Korean Institute of Communications and Information Sciences},
  volume = {47},
  number = {1},
  pages = {79-87},
  month = {Jan}
}
@article{Kim2022Kickboard,
  venue_type = {korean},
  title = {Deep Learning-based Electric Kickboard Helmet Detection in Night Driving Environment},
  author = {Daun Kim and Jin-Woo Jeong},
  year = {2022},
  journal = {The Transactions of the Korean Institute of Electrical Engineers},
  volume = {71},
  number = {10},
  pages = {1411-1419},
  month = {Oct}
}
@inproceedings{Hong2022Implementation,
  venue_type = {korean},
  title = {Implementation of Book Damage Detection Technique Using Deformable DETR},
  author = {Sumin Hong and Mihwan Yu and Jin-Woo Jeong},
  year = {2022},
  booktitle = {한국통신학회 2022 동계},
  award_name = {Outstanding Paper},
  award = {Outstanding Paper}
}
@inproceedings{Yu2022Interactive,
  venue_type = {korean},
  title = {Interactive PIPO Painting Design Web Application Using Multimodal Style Transfer},
  author = {Mihwan Yu and Jin-Woo Jeong},
  year = {2022},
  booktitle = {대한전자공학회 2022 하계}
}
@inproceedings{Oh2022Food,
  venue_type = {korean},
  title = {Food Expiration Date Estimation System Based on Vision Transformer and EasyOCR},
  author = {Jiyeon Oh and Jin-Woo Jeong},
  year = {2022},
  booktitle = {KCC 2022},
  award_name = {Outstanding Paper},
  award = {Outstanding Paper}
}
@inproceedings{Oroceo2022Edge,
  venue_type = {korean},
  title = {Edge Device-based Emotion Recognition Keyword Extraction System},
  author = {Paul Angelo Oroceo and Jungin Kim and Yucheol Jeong and Jin-Woo Jeong and Sangho Kim and Wansu Lim},
  year = {2022},
  booktitle = {대한인간공학회 2022 추계}
}
@inproceedings{Kim2021Web,
  venue_type = {korean},
  title = {Web-based Deep Learning Support System for Object Detection Data Training and Management},
  author = {JuYeop Kim and Min Young Kim and Ji-Yong Jeon and Eunhye Jeong and Sodam Hyeon and Jin-Woo Jeong and Tae-Hyung Kim},
  year = {2021},
  booktitle = {SWCC 2021 하계}
}
@inproceedings{Jeong2021Knock,
  venue_type = {international},
    preview = {Jeong2021Knock.png},
  title = {Knock &Tap: Classification and Localization of Knock and Tap Gestures using Deep Sound Transfer Learning},
  author = {Jae-Yeop Jeong and Jung-Hwa Kim and Ha-Yeong Yoon and Jin-Woo Jeong},
  year = {2021},
  booktitle = {ACM International Conference on Multi-modal Interaction (ICMI)},
  location = {Montreal, Canada},
  month = {Oct},
  doi={10.1145/3461615.3485428}
}
@inproceedings{Kwak2021Deep,
  venue_type = {international},
    preview = {Kwak2021Deep.png},
  title = {Deep Learning-based Safety-Assistant System for Micro-mobility Vehicles: Preliminary Study},
  author = {Hyeon-Seo Kwak and Min-Young Kim and Ji-Yong Jeon and Eun-Hye Jeong and Ju-Yeop Kim and So-Dam Hyeon and Jin-Woo Jeong},
  year = {2021},
  booktitle = {International Conference on Information Networking (ICOIN)},
  location = {Jeju},
  month = {Jan}
}
@inproceedings{Hong2021Proposal,
  venue_type = {korean},
  title = {Proposal of a Deep Learning-based Used Book Damage Detection Technique},
  author = {Sumin Hong and Jung-Hwa Kim and Jin-Woo Jeong},
  year = {2021},
  booktitle = {대한인간공학회 2021 추계}
}
@inproceedings{Kim2021Deep,
  venue_type = {korean},
  title = {Deep Learning-based Effector Classification System for Electric Guitar Players},
  author = {Namsub Kim and Jae-Yeop Jeong and Jin-Woo Jeong},
  year = {2021},
  booktitle = {대한인간공학회 2021 추계}
}
@inproceedings{Kim2021Proposal,
  venue_type = {korean},
  title = {Proposal of Instructor's Gaze Visualization System for Enhancing Learner's Learning Efficiency in Online Education},
  author = {Daun Kim and Namsub Kim and Sumin Hong and Jin-Woo Jeong},
  year = {2021},
  booktitle = {한국통신학회 2021 추계}
}
@article{Ha2020Temporal,
  venue_type = {international},
    preview = {Ha2020Temporal.png},
  title = {Temporal Pyramid Pooling for Decoding Motor-Imagery EEG Signals},
  author = {Kwon-Woo Ha and Jin-Woo Jeong},
  year = {2020},
  journal = {IEEE Access},
  volume = {9},
  pages = {3112-3125}
}
@article{Kim2020Gaze,
  venue_type = {international},
    preview = {Kim2020Gaze.png},
  title = {Gaze in the Dark: Gaze Estimation in a Low-light Environment with Generative Adversarial Networks},
  author = {Jung-Hwa Kim and Jin-Woo Jeong},
  year = {2020},
  journal = {Sensors},
  volume = {20},
  number = {20},
  pages = {4935}
}
@inproceedings{Kim2020GazeDark,
  venue_type = {international},
    preview = {Kim2020GazeDark.png},
  title = {Gaze Estimation in the Dark with Generative Adversarial Networks},
  author = {Jung-Hwa Kim and Jin-Woo Jeong},
  year = {2020},
  booktitle = {ACM ETRA Workshop on Eye Tracking for Quality of Experience in Multimedia (ET-MM'20)},
  location = {Germany},
  month = {Jun},
  doi={10.1145/3517031.3529238}
  }
@inproceedings{Kim2020Preliminary,
  venue_type = {international},
    preview = {Kim2020Preliminary.png},
  abbr = {Top-Tier Conference},
  title = {A Preliminary Study on Performance Evaluation of Multi-View Multi-Modal Gaze Estimation under Challenging Conditions},
  author = {Jung-Hwa Kim and Jin-Woo Jeong},
  year = {2020},
  booktitle = {ACM CHI Conference on Human Factors in Computing Systems (CHI'20) Late-Breaking Works},
  location = {USA},
  month = {Apr}
}
@inproceedings{Jo2020Prediction,
  venue_type = {international},
    preview = {Jo2020Prediction.png},
  title = {Prediction of Visual Memorability with EEG Signals using Deep Neural Networks},
  author = {Sang-Yeong Jo and Jin-Woo Jeong},
  year = {2020},
  booktitle = {The 8th IEEE International Conference on Brain Computer Interface},
  location = {Korea},
  month = {Feb}
}
@article{Jo2020Prediction,
  venue_type = {international},
    preview = {Jo2020Prediction.png},
  title = {Prediction of Visual Memorability with EEG Signals: A Comparative Study},
  author = {Sang-Yeong Jo and Jin-Woo Jeong},
  year = {2020},
  journal = {Sensors},
  volume = {20},
  number = {9},
  pages = {2694}
}
@article{Jang2020Smart,
  venue_type = {korean},
  title = {Design and Implementation of Smart Crosswalk System through Deep Learning-based Vehicle Recognition and Speed Estimation on Edge Devices},
  author = {Sun-Hye Jang and Hee-Eun Cho and Jin-Woo Jeong},
  year = {2020},
  journal = {Journal of the Korea Institute of Information and Communication Engineering},
  volume = {24},
  number = {4},
  pages = {467-473},
  month = {Apr}
}
@article{Kim2020Cultural,
  venue_type = {korean},
  title = {Analysis of Cultural Characteristics of Image Retrieval using Deep Transfer Learning},
  author = {Hyeon-Sik Kim and Jin-Woo Jeong},
  year = {2020},
  journal = {Journal of the Korea Institute of Information and Communication Engineering},
  volume = {24},
  number = {5},
  pages = {674-677},
  month = {Apr}
}
@article{Kim2020Stage,
  venue_type = {korean},
  title = {A Study on Stage Tour Content Generation Method based on Deep Learning Style Transfer},
  author = {Dong-Min Kim and Hyeon-Sik Kim and Dae-Hyun Bong and Jong-Youn Choi and Jin-Woo Jeong},
  year = {2020},
  journal = {Journal of the Korea Institute of Information and Communication Engineering},
  volume = {24},
  number = {11},
  pages = {1403-1410},
  month = {Nov}
}
@inproceedings{Ji2020Emotion,
  venue_type = {korean},
  title = {Emotion Classification Based on EEG and Gaze Data using CapsuleNet},
  author = {Kyu-Bin Ji and Jin-Woo Jeong},
  year = {2020},
  booktitle = {HCI Korea 2020}
}
@inproceedings{Yoon2020Deep,
  venue_type = {korean},
  title = {Deep Learning-based Sidewalk Condition Recognition for Wheelchair Users},
  author = {Ha-Yeong Yoon and Jung-Hwa Kim and Hyeon Seo Kwak and Jin-Woo Jeong},
  year = {2020},
  booktitle = {KSC 2020}
}
@inproceedings{Ha2019Decoding,
  venue_type = {international},
    preview = {Ha2019Decoding.png},
  title = {Decoding Two-Class Motor Imagery EEG with Capsule Networks},
  author = {Kwon-Woo Ha and Jin-Woo Jeong},
  year = {2019},
  booktitle = {The 6th IEEE International Conference on Big Data and Smart Computing (BigComp 2019)},
  location = {Kyoto, Japan},
  month = {Jan}
}
@inproceedings{Jo2019Scenery,
  venue_type = {international},
    preview = {Jo2019Scenery.png},
  title = {Scenery-based Fashion Recommendation with Cross-Domain Generative Adversarial Networks},
  author = {Sang-Young Jo and Sun-Hye Jang and Hee-Eun Cho and Jin-Woo Jeong},
  year = {2019},
  booktitle = {The 6th IEEE International Conference on Big Data and Smart Computing (BigComp 2019)},
  location = {Kyoto, Japan},
  month = {Jan}
}
@article{Ha2019Motor,
  venue_type = {international},
    preview = {Ha2019Motor.png},
  title = {Motor Imagery EEG Classification Using Capsule Networks},
  author = {Kwon-Woo Ha and Jin-Woo Jeong},
  year = {2019},
  journal = {Sensors},
  volume = {19},
  number = {13},
  pages = {2854}
}
@article{Kim2019Watch,
  venue_type = {international},
    preview = {Kim2019Watch.png},
  title = {Watch & Do: A Smart IoT Interaction System with Object Detection and Gaze Estimation},
  author = {Jung-Hwa Kim and Seung-June Choi and Jin-Woo Jeong},
  year = {2019},
  journal = {IEEE Transactions on Consumer Electronics},
  volume = {65},
  number = {2}
}
@article{Hwang2019VisKit,
  venue_type = {international},
    preview = {Hwang2019VisKit.png},
  title = {VisKit: Web-Based Interactive IoT Management with Deep Visual Object Detection},
  author = {Chae-Eun Hwang and Sung-Hun Lee and Jin-Woo Jeong},
  year = {2019},
  journal = {Journal of Sensor and Actuator Networks},
  volume = {8},
  number = {1},
  pages = {12}
}
@inproceedings{Ha2018Decoding,
  venue_type = {international},
    preview = {Ha2018Decoding.png},
  title = {Decoding EEG with Capsules: A Preliminary Study},
  author = {Kwon-Woo Ha and Jin-Woo Jeong},
  year = {2018},
  booktitle = {The 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2018)},
  location = {Honolulu, HI},
  month = {Jul}
}
@article{Lee2018Nools,
  venue_type = {korean},
  title = {Design and Implementation of Nools-based Rule Engine for Intelligent IoE Platform},
  author = {Sung-Hun Lee and Chae-Eun Hwang and Jin-Woo Jeong},
  year = {2018},
  journal = {Journal of the Korea Institute of Information, Electronics, and Communication Technology},
  volume = {11},
  number = {4},
  pages = {379-387},
  month = {Aug}
}
@article{Cho2018Hospital,
  venue_type = {korean},
  title = {Design and Implementation of Hospital Room Management System based on IoT Carebot},
  author = {Sang-Yeong Jo and Jin-Woo Jeong},
  year = {2018},
  journal = {Journal of the Korea Institute of Information, Electronics, and Communication Technology},
  volume = {11},
  number = {4},
  pages = {370-378},
  month = {Aug}
}
@inproceedings{Kim2018IoT,
  venue_type = {korean},
  title = {IoT Control through Deep Learning-based Real-time Object Detection and Gaze Tracking},
  author = {Jung-Hwa Kim and Seung-June Choi and Jin-Woo Jeong},
  year = {2018},
  booktitle = {한국정보기술학회 2018 하계}
}
@article{Baek2017Thing,
  venue_type = {korean},
  preview = {Baek2017Thing.png},
  title = {Thing Control System based on IoT Tags and Social Messages},
  author = {Seung-Min Baek and Yeon-Ju Jin and Kwon-Woo Ha and Sang-Wook Han and Jin-Woo Jeong},
  year = {2017},
  journal = {KIISE Transactions on Computing Practices},
  volume = {23},
  number = {9},
  pages = {550-556},
  month = {Sep}
}
@article{Kim2017Gesture,
  venue_type = {korean},
  preview = {Kim2017Gesture.png},
  title = {Design and Implementation of Gesture-based IoT Control System using Wearable Devices},
  author = {Jun-Su Kim and Seung-Min Baek and Yeon-Ju Jin and Sang-Wook Han and Jin-Woo Jeong},
  year = {2017},
  journal = {Journal of Korean Institute of Information Technology},
  volume = {15},
  number = {10},
  pages = {101-112},
  month = {Oct}
}
@article{Kim2017Crowdsourcing,
  venue_type = {korean},
  preview = {Kim2017Crowdsourcing.png},
  title = {Design and Implementation of Crowdsourcing-based Q&A System using Social Chatbot},
  author = {Bo-Kyung Kim and Hyun-Ah Kim and Chae-Eun Sim and Jin-Woo Jeong},
  year = {2017},
  journal = {Journal of Korean Institute of Information Technology},
  volume = {15},
  number = {11},
  pages = {125-138},
  month = {Nov}
}
@inproceedings{Baek2017Efficient,
  venue_type = {korean},
  title = {Efficient IoT Object Control using Gestures},
  author = {Seung-Min Baek and Yeon-Ju Jin and Kwon-Woo Ha and Sang-Wook Han and Jin-Woo Jeong},
  year = {2017},
  booktitle = {한국정보기술학회 2017 하계}
}
@inproceedings{Lee2017VisKit,
  venue_type = {korean},
  title = {VisKit: Design of a Video-based Intelligent IoT Control System},
  author = {Sung-Hun Lee and Kwon-Woo Ha and Chae-Eun Hwang and Jin-Woo Jeong},
  year = {2017},
  booktitle = {KSC 2017}
}
@article{Jeong2014Automatic,
  venue_type = {international},
    preview = {Jeong2014Automatic.png},
  title = {Automatic image annotation using affective vocabularies: attribute-based learning approach},
  author = {Jin-Woo Jeong and Dong-Ho Lee},
  year = {2014},
  journal = {Journal of Information Science},
  volume = {40},
  number = {4},
  pages = {426-445}
}
@article{Jeong2014Inferring,
  venue_type = {international},
    preview = {Jeong2014Inferring.png},
  title = {Inferring Search Intents from Remote Control Movement Patterns: A New Content Search Method for Smart TV},
  author = {Jin-Woo Jeong and Dong-Ho Lee},
  year = {2014},
  journal = {IEEE Transactions on Consumer Electronics},
  volume = {60},
  number = {1},
  pages = {92-98}
}
@article{Jeong2013Emotion,
  venue_type = {international},
  preview = {Jeong2013Emotion.png},
  title = {Emotion-based Image Retrieval and Annotation Method using Attribute Information of Objects},
  author = {Jin-Woo Jeong and Dong-Ho Lee},
  year = {2013},
  journal = {Database Research},
  volume = {29},
  number = {1},
  pages = {17-36},
  month = {Apr}
}
@article{Qasim2013ConceptMap,
  venue_type = {international},  
  title = {Design and Implementation of Concept Map Construction System using Message Passing Technique},
  author = {Iqbal Qasim and Jin-Woo Jeong and Ji-Woog Heo and Dong-Ho Lee},
  year = {2013},
  journal = {Database Research},
  volume = {29},
  number = {1},
  pages = {51-71},
  month = {Apr}
}
@article{Qasim2013Concept,
  venue_type = {international},    
  title = {Concept map construction from text documents using affinity propagation},
  author = {lqbal Qasim and Jin-Woo Jeong and Jee-Uk Heu and Dong-Ho Lee},
  year = {2013},
  journal = {Journal of Information Science},
  volume = {39},
  number = {6},
  pages = {719-736}
}
@article{Jeong2013i,
  venue_type = {international},
    preview = {Jeong2013i.png},
  title = {i-TagRanker: an efficient tag ranking system for image sharing and retrieval using the semantic relationships between tags},
  author = {Jin-Woo Jeong and Hyun-Ki Hong and Dong-Ho Lee},
  year = {2013},
  journal = {Multimedia Tools and Applications},
  volume = {62},
  number = {2},
  pages = {451-478}
}
@inproceedings{Jeong2013Crowd,
  venue_type = {international},
    preview = {Jeong2013Crowd.png},
  title = {A Crowd-Powered Socially Embedded Search Engine},
  author = {Jin-Woo Jeong and Meredith Morris and Jaime Teevan and Daniel Liebling},
  year = {2013},
  booktitle = {The Seventh International AAAI Conference on Weblogs and Social Media (ICWSM 2013)},
  location = {Boston, MA},
  month = {Jul}
}
@inproceedings{Jeong2013Understanding,
  venue_type = {international},
    preview = {Jeong2013Understanding.png},
  title = {Understanding the Potential of Social Questions in the Web Search},
  author = {Jin-Woo Jeong and Jee-Uk Heu and Dong-Ho Lee},
  year = {2013},
  booktitle = {ACM International Conference on Computer Supported Cooperative Work and Social Computing (CSCW)},
  location = {USA},
  month = {Feb},
  note = {Top 10 of Human Computer Interaction at Google Scholar Metrics}
}
@inproceedings{Heu2013Multi,
  venue_type = {international},
    preview = {Heu2013Multi.png},
  title = {Multi-Document Summarization Exploiting Semantic Analysis based on Tag Cluster},
  author = {Jee-Uk Heu and Jin-Woo Jeong and Iqbal Qasim and Young-Do Joo and Joon-Myun Cho and Dong-Ho Lee},
  year = {2013},
  booktitle = {19th International Conference on Multimedia Modeling (MMM)},
  location = {China},
  month = {Jan}
}
@inproceedings{Jeong2012Towards,
  venue_type = {international},
    preview = {Jeong2012Towards.png},
  title = {Towards Measuring the Visualness of a Concept},
  author = {Jin-Woo Jeong and Xin-Jing Wang and Dong-Ho Lee},
  year = {2012},
  booktitle = {ACM International Conference on Information and Knowledge Management (CIKM)},
  location = {USA},
  month = {Oct},
  note = {Top 10 of Data Mining at Microsoft Academic Ranking}
}
@inproceedings{Jeong2012Visual,
  venue_type = {international},
    preview = {Jeong2012Visual.png},
  title = {Visual Summarization of the Social Image Collection using Image Attractiveness Learned from Social Behaviours},
  author = {Jin-Woo Jeong and Hyun-Ki Hong and Jee-Uk Heu and Iqbal Qasim and Dong-Ho Lee},
  year = {2012},
  booktitle = {IEEE International Conference on Multimedia & Expo (ICME)},
  location = {Australia},
  month = {Jul},
  note = {Top 10 of Multimedia at Microsoft Academic Ranking}
}
@inproceedings{Jeong2011Exploiting,
  venue_type = {international},
    preview = {Jeong2011Exploiting.png},
  title = {Exploiting of Flickr Note and Its Applications for Social Image Sharing and Search},
  author = {Jin-Woo Jeong and Hyun-Ki Hong and Dong-Ho Lee},
  year = {2011},
  booktitle = {IEEE International Symposium on Multimedia (ISM)},
  location = {USA},
  month = {Dec}
}
@inproceedings{Qasim2011Exploiting,
  venue_type = {international},
    preview = {Qasim2011Exploiting.png},
  title = {Exploiting Affinity Propagation for Automatic Acquisition of Domain Concept in Ontology Learning},
  author = {Iqbal Qasim and Jin-Woo Jeong and Sharifullah Khan and Dong-Ho Lee},
  year = {2011},
  booktitle = {7th IEEE International Conference on Emerging Technologies (ICET)},
  location = {Pakistan},
  month = {Sep}
}
@inproceedings{Jeong2011Ontology,
  venue_type = {international},
    preview = {Jeong2011Ontology.png},
  title = {Ontology-based Automatic Video Annotation Technique in Smart TV Environment},
  author = {Jin-Woo Jeong and Hyun-Ki Hong and Dong-Ho Lee},
  year = {2011},
  booktitle = {IEEE Transactions on Consumer Electronics},
  volume = {57},
  number = {4},
  pages = {1830-1836}
}
@article{Hong2010Efficient,
  venue_type = {international},
  preview = {Hong2010Efficient.png},
  title = {Efficient Tag Ranking Method based on Semantic Relationships between Image Tags},
  author = {Hyun-Ki Hong and Jin-Woo Jeong and Dong-Ho Lee},
  year = {2010},
  journal = {Database Research},
  volume = {26},
  number = {3},
  pages = {31-36},
  month = {Dec}
}
@inproceedings{Park2007OLYBIA,
  venue_type = {international},
    preview = {Park2007OLYBIA.png},
  title = {OLYBIA : Ontology-based Automatic Image Annotation System using Semantic Inference Rules},
  author = {Kyung-Wook Park and Jin-Woo Jeong and Dong-Ho Lee},
  year = {2007},
  booktitle = {Lecture Note in Computer Science (LNCS)},
  volume = {4443},
  pages = {485-496}
}
@inproceedings{Jeong2007Automatic,
  venue_type = {international},
    preview = {Jeong2007Automatic.png},
  title = {Automatic Extraction of Semantic Relationships from Images Using Ontologies and SVM Classifiers},
  author = {Jin-Woo Jeong and Kyung-Wook Park and OukSeh Lee and Dong-Ho Lee},
  year = {2007},
  booktitle = {Lecture Notes in Computer Science(LNCS)},
  volume = {4577},
  pages = {184-194}
}