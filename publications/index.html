<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publication | Interaction Lab. </title> <meta name="author" content="Interaction Lab."> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://st-ixlab.github.io/publications/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Interaction</span> Lab. </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">News </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">Members </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publication <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/activities/">Activities </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publication</h1> <p class="post-description"></p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">Under Review</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TBD2024Enhancing" class="col-sm-8"> <div class="title">Enhancing Multitasking in Mixed Reality: Design and Assessment of Visual Aids for Managing Interruptions</div> <div class="author"> TBD </div> <div class="periodical"> </div> <div class="periodical"> Under review </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TBD2023MuNext" class="col-sm-8"> <div class="title">MuNext: Multi-task UNeXt for Efficient Breast Cancer Classification and Segmentation</div> <div class="author"> TBD </div> <div class="periodical"> </div> <div class="periodical"> Under review </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TBD2023Classification" class="col-sm-8"> <div class="title">Classification of Breast Cancer from Mammograms: Do Vision Transformers and MLP-Mixers Outperform CNNs?</div> <div class="author"> TBD </div> <div class="periodical"> </div> <div class="periodical"> Under review </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TBD2023Efficient" class="col-sm-8"> <div class="title">Efficient Multi-Task Learning for Facial Expression Recognition In-the-wild</div> <div class="author"> TBD </div> <div class="periodical"> </div> <div class="periodical"> Under review </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">Publications</h2> <h2 class="bibliography">2026</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Journal</abbr> <figure> <picture> <img src="/assets/img/publication_preview/OH2026103673.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="OH2026103673.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="OH2026103673" class="col-sm-8"> <div class="title">SnapSound: Empowering everyone to customize sound experience with Generative AI</div> <div class="author"> Jiyeon Oh and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>International Journal of Human-Computer Studies</em>, 2026 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.ijhcs.2025.103673" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The rise of user-centric experiences in the digital landscape has led to a surge in demand for personalized multimedia content. Users now seek to customize not only visual but also auditory components to suit their preferences. In this context, sound design plays a crucial role, enabling users to tailor audio experiences accordingly. However, its inherent complexity poses various challenges, particularly for non-expert users. To address this challenge, we introduce SnapSound, a novel assistive system designed specifically for non-experts in sound design for video content. Our system leverages generative AI to streamline the sound design process and offers intuitive tools for sound selection, synchronization, and seamless integration with visuals. Through a user study, we evaluate SnapSound’s usability and effectiveness compared to manual editing. Furthermore, our study provides valuable insights and design recommendations for enhancing user experience of future AI-based sound design systems. This work represents a significant step forward in empowering non-experts to easily customize their auditory experiences.</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Min2025Silent.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Min2025Silent.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Min2025Silent" class="col-sm-8"> <div class="title">Silent Yet Expressive: Toward Seamless VR Communication through Emotion-aware Silent Speech Interfaces</div> <div class="author"> Yewon Min, Jiyeon Oh, Jae-Yeop Jeong, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM Symposium on User Interface Software and Technology (UIST) Posters</em>, Busan, Korea, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3746058.3758359" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The paper addresses the fundamental challenge of building affective intelligent systems that are aware of the emotions of those communicating. It highlights that emotion is a complex mental state influenced by external events, physiological changes, or relationships. The core aim of the research is the development of an unobtrusive emotion recognition approach and a user scenario for emotion-oriented social communication in VR, enabling users to communicate silently with computing devices without audible speech or discernible movements.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Min2025Exploring.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Min2025Exploring.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Min2025Exploring" class="col-sm-8"> <div class="title">Exploring Emotion Expression Through Silent Speech Interface in Public VR/MR: Effects of Automation on User Experience</div> <div class="author"> Yewon Min, Jiyeon Oh, Jae-Yeop Jeong, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM CHI Conference on Human Factors in Computing Systems (ACM CHI) Late-Breaking Works</em>, Yokohama, Japan, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3706599.3720209" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Silent speech input offers a secure and private alternative to voice input in public spaces. As HMDs become increasingly mobile, the need for integrating silent speech recognition grows to enable seamless interaction in public settings. However, the potential of silent speech-based emotion recognition in VR/MR environments remains underexplored, despite the importance of emotional expression in social interactions. This paper evaluates how different levels of automation in emotional expression—Manual, Semi-auto, and Automatic—affect user experience in VR environments. The results revealed that Semi-auto methods generally offer better efficiency, user control, and socially acceptable interactions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2025Understanding.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2025Understanding.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2025Understanding" class="col-sm-8"> <div class="title">Understanding User Behavior in Window Selection using Dragging for Multiple Targets</div> <div class="author"> Jae-Yeop Jeong and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM CHI Conference on Human Factors in Computing Systems (ACM CHI)</em>, Yokohama, Japan, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Honorable Mention</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3706598.3713410" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Honorable Mention</p> </div> <div class="abstract hidden"> <p>Window selection is a fundamental part of graphical user interfaces (GUIs). Although the dragging technique is widely used for selecting multiple items, its application in window selection has not been thoroughly studied. This paper presents an experimental study investigating user behavior in window selection using dragging for multiple targets. We analyze the performance and preferences of users, providing insights into improving the design of window selection interactions. Our findings highlight the effectiveness of dragging for selecting multiple windows and suggest design guidelines for enhancing user experience in multi-window environments.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Kim2025Through.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2025Through.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2025Through" class="col-sm-8"> <div class="title">Through the Looking Glass, and What We Found There: A Comprehensive Study of User Experiences with Pass-Through Devices in Everyday Activities</div> <div class="author"> Daewook Kim, Yewon Min, Jae-Yeop Jeong, Sehee Han, JiYeon Hwang, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM CHI Conference on Human Factors in Computing Systems (ACM CHI)</em>, Yokohama, Japan, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3706598.3714221" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Pass-through technologies are promising for mixed reality (MR) systems. Therefore, various MR applications operating in pass-through devices emerged in diverse domains, such as education and healthcare. However, research on the everyday use of pass-through devices remains limited, despite it blending real and virtual environments. This study explores the user experience of pass-through devices in people’s daily tasks. We conducted a field study with 16 participants and analyzed data from eight daily tasks. For in-depth analysis, we employed three measures in terms of quantitative, qualitative, and bio-signal. As a result, we found that participants felt differently in terms of immersion, collision anxiety, and workload. Findings suggest that pass-through devices are not yet fully ready for integration into daily life. However, the potential for widespread adoption exists as the technology continues to advance. Finally, we offer guidelines and considerations to improve the usability of pass-through devices for everyday use.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Lukianova2025Picture.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Lukianova2025Picture.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Lukianova2025Picture" class="col-sm-8"> <div class="title">A picture is worth a thousand words? Investigating the Impact of Image Aids in AR on Memory Recall for Everyday Tasks</div> <div class="author"> Elizaveta Lukianova, Jae-Yeop Jeong, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM Conference on Intelligent User Interfaces (ACM IUI)</em>, Cagliari, Italy, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Best Paper</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3708359.3712087" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Best Paper</p> </div> <div class="abstract hidden"> <p>This paper investigates the impact of image aids in AR on memory recall for everyday tasks. It explores how Augmented Reality (AR) can enhance learning by supporting embodied interaction, contextual immersion, and multimodal engagement. The study suggests that image aids in AR can significantly improve memory retention and facilitate the comprehension of abstract concepts.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2025Systematic" class="col-sm-8"> <div class="title">A Systematic Literature Review on Personality Modeling and Expression for Social Robot Personalization Services</div> <div class="author"> Shee-Ihn Kim and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Journal of the Korea Academia-Industrial cooperation Society</em>, Mar 2025 </div> <div class="periodical"> Accepted </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Han2025Exploring" class="col-sm-8"> <div class="title">Exploring the Effects of Immediate and Gradual UX Transitions during Short-form Video Viewing on Time Perception and Emotional Response</div> <div class="author"> Sehee Han, Jiyeon Oh, Yewon Min, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In HCI Korea 2025</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Hwang2025Personal" class="col-sm-8"> <div class="title">Personal Baseball: LLM-based User-Customized Baseball Highlight Video Generation System</div> <div class="author"> Jiyeon Hwang, Jiyeon Oh, Dae Wook Kim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In HCI Korea 2025</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/11236281.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="11236281.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="11236281" class="col-sm-8"> <div class="title">Exploring Interface Design of Translation System for Enhanced Immersion and Usability in Mixed Reality</div> <div class="author"> Jiyeon Oh and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 2025 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct)</em>, Mar 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1109/ISMAR-Adjunct68609.2025.00156" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Choi2024Eeg.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Choi2024Eeg.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Choi2024Eeg" class="col-sm-8"> <div class="title">EEG Dataset for the Recognition of Different Emotions Induced in Voice-User Interaction</div> <div class="author"> Ga-Young Choi, Jong-Gyu Shin, Ji-Yoon Lee, Jun-Seok Lee, In-Seok Heo, Ha-Yeong Yoon, Wansu Lim, <em class="author-self">Jin-Woo Jeong</em>, Sang-Ho Kim, and Han-Jeong Hwang </div> <div class="periodical"> <em>Scientific Data</em>, Mar 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1038/s41597-024-03887-9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Electroencephalography (EEG)-based open-access datasets are available for emotion recognition studies, typically using external auditory or visual stimuli to artificially evoke predefined emotions. This study introduces a novel EEG dataset that captures emotional information induced during a realistic human-computer interaction (HCI) with a voice user interface system, designed to mimic natural human-to-human communication. To validate this dataset, researchers applied a series of signal processing and machine learning methods to the EEG data for neurophysiological investigation and binary emotion classification. The maximum classification accuracy achieved ranged from 43.3% to 90.8% across 38 subjects, and the classification features could be interpreted neurophysiologically. This EEG data has the potential to contribute to the development of reliable HCI systems.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Oh2024Mitigating.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Oh2024Mitigating.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Oh2024Mitigating" class="col-sm-8"> <div class="title">Mitigating Inappropriate Concepts in Text-to-Image Generation with Attention-guided Image Editing</div> <div class="author"> Jiyeon Oh, Jae-Yeop Jeong, Yeong-Gi Hong, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>PeerJ Computer Science</em>, Mar 2024 </div> <div class="periodical"> Accepted </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.7717/peerj-cs.3170" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Text-to-image generative models have recently garnered a significant surge due to their ability to produce diverse images based on given text prompts. However, concerns regarding the occasional generation of inappropriate, offensive, or explicit content have arisen. To address this, a simple yet effective method is proposed that leverages attention maps to selectively suppress inappropriate concepts during image generation. Unlike existing approaches that often sacrifice original image context or demand substantial computational overhead, this method preserves image integrity without requiring additional model training or extensive engineering effort. To evaluate the method, comprehensive quantitative assessments were conducted on inappropriateness reduction, text fidelity, image consistency, and computational cost, alongside an online human perceptual study involving 20 participants. The results from the statistical analysis demonstrated that the method effectively removes inappropriate content while preserving the integrity of the original images with high computational efficiency.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2024DesigningLLM.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2024DesigningLLM.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2024DesigningLLM" class="col-sm-8"> <div class="title">Designing LLM Response Layouts for XR Workspaces in Vehicles</div> <div class="author"> Daun Kim and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM SIGGRAPH ASIA</em>, Tokyo, Japan, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3681756.3697877" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This study investigates how large language model (LLM) responses can be effectively presented in Extended Reality (XR) environments within vehicles. The research highlights usability challenges associated with current linear layouts in XR, such as difficulties with extensive scrolling and mid-air interactions. To address these issues, the authors propose an improved layout design that utilizes multiple windows for efficient access to diverse content. This new layout also features a cohesive placement strategy aimed at minimizing motion sickness. A user study involving 24 participants was conducted to compare this proposed interface with a conventional web browser interface across various information-seeking tasks. The paper aims to inform the design of more effective and comfortable LLM response layouts for in-vehicle XR workspaces.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Min2024Public.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Min2024Public.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Min2024Public" class="col-sm-8"> <div class="title">Public Speaking Q&amp;A Practice with LLM-Generated Personas in Virtual Reality</div> <div class="author"> Yewon Min and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In IEEE International Symposium on Mixed and Augmented Reality (ISMAR ’24)</em>, Seattle, USA, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMAR-Adjunct64951.2024.00143" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper introduces a novel Virtual Reality (VR)-based Q&amp;A practice system that leverages the power of Large Language Models (LLMs). The system aims to support Q&amp;A practice for upcoming public speaking engagements by offering an immersive VR training environment. This environment is populated with LLM-generated audiences, each designed to pose diverse and realistic questions based on distinct personas. A pilot user study was conducted with 20 participants who engaged in these VR-based Q&amp;A practice sessions. During these sessions, participants encountered a variety of questions related to their provided presentation material, all of which were generated by the LLM-based personas. Through post-surveys and interviews, the study evaluated the effectiveness of this proposed method. Participants valued the system for its engagement and ability to foster focus, though they also identified areas for improvement. The study ultimately demonstrated the potential of integrating VR and LLMs to create a powerful and immersive tool for Q&amp;A practice.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2024DesigningResponse.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2024DesigningResponse.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2024DesigningResponse" class="col-sm-8"> <div class="title">Designing Response Layouts of LLM-based Conversational Agents for Extended Reality Environments in Vehicles</div> <div class="author"> Daun Kim and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In Technical Congress of the International Ergonomics Association (IEA 24)</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2024Effect.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2024Effect.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2024Effect" class="col-sm-8"> <div class="title">Effect of Onset Position of Ray Casting in Virtual Reality</div> <div class="author"> Jae-Yeop Jeong and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM SIGCHI Conference on Human Factors in Computing Systems (CHI’24) Late-Breaking Works</em>, Hawaiʻ, USA, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3613905.3650905" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>In virtual reality (VR), interaction techniques significantly influence user experience and performance. This study explores the effect of the onset position of ray casting in VR through a target selection task. Researchers designed a pointing task (specifically, a Fitts’ law task) to collect data with various onset positions. The data was then analyzed to determine how different onset positions affect individual performance for 3D interaction in terms of pointing and selection. The results indicated significant effects for each participant, but no generalized impact was found across all participants. These findings highlight the complexity of human-computer interaction in ray casting and suggest that a one-size-fits-all approach may not be effective. For future research and practical applications, the study advocates a personalized approach that considers the specific preferences and responses of individual users. This tailored strategy could potentially improve the effectiveness and usability of interactive systems that utilize ray cast techniques.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2024Development" class="col-sm-8"> <div class="title">Development of Deep Learning-based Real-time Learner State Recognition and Visualization System for Enhancing Immersive Content Learning Experience</div> <div class="author"> Daun Kim, Ha-Neul Kim, Yewon Min, Se-Yeon Oh, Chae-Rin Lee, Yujin Lee, Jeong-Won Lee, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In HCI Korea 2024</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lukianova2024Study" class="col-sm-8"> <div class="title">Study on the Feedback Modes of Memory Augmentation System for Everyday Life</div> <div class="author"> Lukianova Elizaveta and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 한국통신학회 인공지능학술대회 2024</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Oh2024Attention" class="col-sm-8"> <div class="title">Attention Map-based Technique Proposal and User Evaluation for Reducing Inappropriateness in Text-to-Image Generation</div> <div class="author"> Jiyeon Oh, Jae-Yeop Jeong, Yeong-Gi Hong, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 대한인간공학회 2024 추계</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lukianova2024Preliminary" class="col-sm-8"> <div class="title">A Preliminary Study on Real-time Memory Augmentation via Image and Text-based Feedback for Everyday Tasks using Multimodal Large Language Models</div> <div class="author"> Lukianova Elizaveta and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 대한인간공학회 2024 추계</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Journal</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2023EchoTap.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2023EchoTap.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2023EchoTap" class="col-sm-8"> <div class="title">EchoTap: Non-verbal Sound Interaction with Knock and Tap Gestures</div> <div class="author"> Jae-Yeop Jeong, Daun Kim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>International Journal of Human-Computer Interaction</em>, May 2023 </div> <div class="periodical"> Accepted </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1080/10447318.2024.2348837" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>The growing demand for highly accessible interaction technologies to effectively interact with smart devices has led to the increasing popularity of voice user interfaces (VUIs). However, VUIs face interpretation challenges stemming from the variability of natural language input, such as speech clarity issues, linguistic variability, and speech impediments. As an alternative, non-verbal sound-based interaction techniques emerge as highly advantageous for smart device control, mitigating the inherent challenges of VUIs. This article introduces EchoTap, a novel audio interface that harnesses the distinctive sound responses generated by knock and tap gestures on target objects. Employing deep neural networks, EchoTap recognizes both the type and location of these gestures based on their unique sound signatures. Through offline evaluation, EchoTap demonstrated competitive classification accuracy (88% on average) and localization precision (93% on average). Moreover, a user study involving 12 participants validated EchoTap’s practical effectiveness and user-friendliness in real-world scenarios. This study highlights EchoTap’s potential for various daily interaction contexts and discusses further design implications for leveraging auditory interfaces based on simple gestures.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Journal</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2023Enhancing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2023Enhancing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2023Enhancing" class="col-sm-8"> <div class="title">Enhancing Learner Experience with Instructor Cues in Video Lectures: A Comprehensive Exploration and Design Discovery toward A Novel Gaze Visualization</div> <div class="author"> Jae-Yeop Jeong, JiYeon Oh, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Education and Information Technologies</em>, May 2023 </div> <div class="periodical"> Accepted </div> <div class="links"> <a href="https://doi.org/10.1007/s10639-024-12697-w" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2023EmoFlow.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2023EmoFlow.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2023EmoFlow" class="col-sm-8"> <div class="title">EmoFlow: Visualizing Emotional Changes in Video Chat – Preliminary Study</div> <div class="author"> Daun Kim, Ji Yeon Oh, Jae-Yeop Jeong, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM International Conference on Mobile Human Computer Interaction (MobileHCI) Late-Breaking Works</em>, Athens, Greece, Sep 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3565066.3608702" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>We propose EmoFlow, a prototype service that can recognize the emotional changes of users in video chat and provide a text-based summary along with various visualizations. We present design concepts to represent users’ emotional flow on the mobile messenger application in three types: Emoji, Color, and Shape visualizations. Through a user study with 60 voluntary participants, we found that the Emoji type received the highest score in terms of conveying emotions and design preferences. Also, 65 percent of the participants responded they are willing to use the proposed service if several points are enhanced in the future. The survey results suggest that the proposed service would help users understand and manage their emotional status.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Hong2023Dynamic.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Hong2023Dynamic.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Hong2023Dynamic" class="col-sm-8"> <div class="title">Dynamic Noise Injection for Facial Expression Recognition In-the-Wild</div> <div class="author"> SangHwa Hong and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In IEEE/CVF CVPR workshop on Affective Behavior Analysis in-the-wild</em>, Vancouver, Canada, Jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="abstract hidden"> <p>Facial expression-based emotion analysis is a crucial area of research in artificial intelligence. However, many current works are hindered by low classification or regression performance due to overfitting. To address this, the paper introduces a novel noise injection technique. Specifically, this method, built upon the ResNet-18 architecture, dynamically injects feature-level noise into the BN+ReLU unit to facilitate the learning of more robust features. Experiments conducted on facial expression classification using the AffectNet dataset have demonstrated the effectiveness of this proposed approach. The paper also highlights that, unlike traditional noise injection techniques where noise is continuously introduced, their approach proposes dynamically injecting noise during the training process for selected epochs.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Oh2023Tingle.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Oh2023Tingle.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Oh2023Tingle" class="col-sm-8"> <div class="title">Tingle Just for You: A Preliminary Study of AI-based Customized ASMR Content Generation</div> <div class="author"> Ji Yeon Oh, Daun Kim, Jae-Yeop Jeong, Elizaveta Lukianova, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM SIGCHI Conference on Human Factors in Computing Systems (CHI’23) Late-Breaking Works</em>, Germany, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3544549.3585872" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper discusses research into Autonomous Sensory Meridian Response (ASMR) and the development of AI to generate personalized ASMR content. It explores how ASMR, characterized by pleasant tingling sensations and feelings of relaxation, can be customized using AI. It notes that ASMR videos are not just about tingling, but also offer experiences of social connection, physical intimacy, and activity observation. The study emphasizes the potential for designing video-based applications that incorporate ASMR effects.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Hong2023Spot.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Hong2023Spot.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Hong2023Spot" class="col-sm-8"> <div class="title">Spot The Difference: AI, Please Make This for Me!</div> <div class="author"> Yeong-Gi Hong, Jae-Yeop Jeong, Elizaveta Lukianova, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM SIGCHI Conference on Human Factors in Computing Systems (CHI’23) Late-Breaking Works</em>, Germany, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1145/3544549.3585879" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p> This paper introduces a novel AI-driven framework that streamlines the generation of "spot the difference" challenges. By leveraging advanced generative AI models, such as text-to-image systems and image-to-image modification tools, the system can create a base image from a user-defined prompt and subsequently introduce specific, controlled alterations to generate a second, divergent image. This methodology significantly reduces the need for human intervention in content creation, enabling the production of a vast and diverse array of puzzles. Furthermore, the framework incorporates computer vision techniques to identify and manage these differences, ensuring the integrity and playability of the generated games.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Hong2023Automatic.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Hong2023Automatic.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Hong2023Automatic" class="col-sm-8"> <div class="title">Automatic Examination of Condition of Used Books with YOLO-based Object Detection Framework</div> <div class="author"> Sumin Hong and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Computer Systems Science and Engineering</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.32604/csse.2023.038319" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Yoon2023Deep" class="col-sm-8"> <div class="title">Deep Learning-based Event Recognition Technology in Mobile 360 Video for Expanding Social Safety Net</div> <div class="author"> Ha-Yeong Yoon, Daun Kim, Jiyeon Oh, Sumin Hong, Jiheun Seo, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In HCI Korea 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Yoon2023Comparative" class="col-sm-8"> <div class="title">Comparative Experiment of Breast Cancer Classification for Deep Learning-based Models Considering Mobile Environments</div> <div class="author"> Ha-Yeong Yoon, Seun Choi, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In HCI Korea 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jeong2023Deep" class="col-sm-8"> <div class="title">Deep Learning-based User Emotional Satisfaction Evaluation Model Construction and Pruning Method for User-Responsive Conversational Agents</div> <div class="author"> Jae-Yeop Jeong, Yeong-Gi Hong, Wansu Lim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In HCI Korea 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Hong2023Deep" class="col-sm-8"> <div class="title">Deep Learning Model for Detecting Risky Behavior of Children in Vehicles</div> <div class="author"> Sang-Hwa Hong and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In HCI Korea 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2023Understanding" class="col-sm-8"> <div class="title">Understanding User’s Spatial Context Using Background Noise in Voice Call Environment</div> <div class="author"> Doyoung Kim, Daun Kim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In KCC 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Outstanding Paper</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstanding Paper</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2023Development" class="col-sm-8"> <div class="title">Development of Deep Audio Transfer Learning-based Gesture Classification and Localization Model for Interaction with Immersive Virtual Reality</div> <div class="author"> Namsub Kim, Jae-Yeop Jeong, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In KCC 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Min2023Transportation" class="col-sm-8"> <div class="title">Transportation Vulnerable Detection System for Passenger Safety in Autonomous Buses</div> <div class="author"> Yewon Min, Jiyeon Oh, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In KCC 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Hong2023Estimation" class="col-sm-8"> <div class="title">Estimation of Appropriateness of CPR Compression Frequency and Depth based on HoloLens 2</div> <div class="author"> Sang-Hwa Hong and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In KCC 2023</em>, Apr 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Outstanding Paper</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstanding Paper</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Pascual2022Light.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Pascual2022Light.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Pascual2022Light" class="col-sm-8"> <div class="title">Light-FER: A Lightweight Facial Emotion Recognition System on Edge Devices</div> <div class="author"> Alexander M Pascual, Erick C Valverde, Jeong-In Kim, <em class="author-self">Jin-Woo Jeong</em>, Yuchul Jung, Sang-Ho Kim, and Wansu Lim </div> <div class="periodical"> <em>Sensors</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3390/s22239524" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Facial emotion recognition (FER) systems are crucial in advanced artificial intelligence (AI) applications for enhanced human-computer interactions. Most deep learning-based FER systems face challenges due to low accuracy and high resource demands, particularly when implemented on edge devices with limited computing power and memory. To address these issues, this paper introduces Light-FER, a lightweight FER system derived from the Xception model through model compression techniques. The process involves several steps: First, pruning is applied during network training to eliminate less significant connections within the Xception architecture. Second, the model is quantized to a half-precision format, substantially reducing its memory footprint. Third, various deep learning compilers are benchmarked to identify and utilize advanced optimization techniques for accelerating the FER system’s inference speed. Finally, to experimentally validate the proposed system’s objectives on edge devices, Light-FER is deployed on an NVIDIA Jetson Nano.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Mudeng2022Simply.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Mudeng2022Simply.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Mudeng2022Simply" class="col-sm-8"> <div class="title">Simply Fine-Tuned Deep Learning-Based Classification for Breast Cancer with Mammograms</div> <div class="author"> Mudeng V, <em class="author-self">Jin-Woo Jeong</em>, and Se-woon Choe </div> <div class="periodical"> <em>CMC-Computers, Materials &amp; Continua</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.32604/cmc.2022.031046" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Ayana2022Novel.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Ayana2022Novel.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ayana2022Novel" class="col-sm-8"> <div class="title">A Novel Multistage Transfer Learning for Ultrasound Breast Cancer Image Classification</div> <div class="author"> Gelan Ayana, Jinhyung Park, <em class="author-self">Jin-Woo Jeong</em>, and Se-woon Choe </div> <div class="periodical"> <em>Diagnostics</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3390/diagnostics12010135" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>Breast cancer diagnosis is one of the many areas that has taken advantage of artificial intelligence to achieve better performance, despite the fact that the availability of a large medical image dataset remains a challenge. Transfer learning (TL) is a phenomenon that enables deep learning algorithms to overcome the issue of shortage of training data in constructing an efficient model by transferring knowledge from a given source task to a target task. However, in most cases, ImageNet (natural images) pre-trained models, which do not include medical images, are utilized for transfer learning to medical images. Considering the utilization of microscopic cancer cell line images that can be acquired in large amounts, the authors argue that learning from both natural and medical datasets improves performance in ultrasound breast cancer image classification. The proposed multistage transfer learning (MSTL) algorithm was implemented using three pre-trained models: EfficientNetB2, InceptionV3, and ResNet50 with three optimizers: Adam, Adagrad, and stochastic gradient descent (SGD). Dataset sizes of 20,400 cancer cell images, 200 ultrasound images from Mendeley, and 400 ultrasound images from the MT-Small-Dataset were used. ResNet50-Adagrad-based MSTL achieved a test accuracy of 99 ± 0.612% on the Mendeley dataset and 98.7 ± 1.1% on the MT-Small-Dataset, averaging over 5-fold cross-validation. A p-value of 0.01191 was achieved when comparing MSTL against ImageNet based TL for the Mendeley dataset. The result is a significant improvement in the performance of artificial intelligence methods for ultrasound breast cancer classification compared to state-of-the-art methods and could remarkably improve the early diagnosis of breast cancer in young women.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Choe2022Automatic.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Choe2022Automatic.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Choe2022Automatic" class="col-sm-8"> <div class="title">Automatic cancer cell taxonomy using an ensemble of deep neural networks</div> <div class="author"> Se-woon Choe, Ha-Yeong Yoon, Jae-Yeop Jeong, Jinhyung Park, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Cancers</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.3390/cancers14092224" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2022Ensemble.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2022Ensemble.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2022Ensemble" class="col-sm-8"> <div class="title">Ensemble of Multi-task Learning Networks for Facial Expression Recognition In-the-Wild with Learning from Synthetic Data</div> <div class="author"> Jae-Yeop Jeong, Yeong-Gi Hong, Sumin Hong, JiYeon Oh, Yuchul Jung, Sang-Ho Kim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In IEEE/CVF ECCV workshop on Affective Behavior Analysis in-the-wild</em>, Israel, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/978-3-031-25075-0_5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> <div class="abstract hidden"> <p>This paper presents a computationally efficient ensemble of multi-task learning networks for facial expression recognition in-the-wild. The approach addresses the challenges of variability in head poses, occlusions, and illumination conditions often encountered in real-world scenarios. The proposed method utilizes an ensemble of networks, each trained on different tasks related to facial analysis, to improve the robustness and accuracy of facial expression recognition. Furthermore, an analysis of space and time complexity was conducted.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Yoon2022Classification.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Yoon2022Classification.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yoon2022Classification" class="col-sm-8"> <div class="title">Classification of Breast Cancer Images with Vision Transformers</div> <div class="author"> Ha-Yeong Yoon, Se-woon Choe, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In IEEE International Engineering in Medicine and Biology Conference (EMBC)</em>, Glasgow, UK, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2022Classification.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2022Classification.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2022Classification" class="col-sm-8"> <div class="title">Classification of Facial Expression In-the-Wild based on Ensemble of Multi-head Cross Attention Networks</div> <div class="author"> Jae-Yeop Jeong, Yeong-Gi Hong, Daun Kim, Yuchul Jung, Sang-Ho Kim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In IEEE/CVF CVPR workshop on Affective Behavior Analysis in-the-wild</em>, New Orleans, US, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2022Visualizing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2022Visualizing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2022Visualizing" class="col-sm-8"> <div class="title">Visualizing Instructor’s Gaze Information for Online Video-based Learning: Preliminary Study</div> <div class="author"> Daun Kim, Jae-Yeop Jeong, Sumin Hong, Namsub Kim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM ETRA workshop on Eye Tracking in Learning and Education</em>, Seattle, US, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3517031.3529238" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Yoon2022Classification.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Yoon2022Classification.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Yoon2022Classificatioo" class="col-sm-8"> <div class="title">Classification of the Sidewalk Condition Using Self-Supervised Transfer Learning for Wheelchair Safety Driving</div> <div class="author"> Ha-Yeong Yoon, Jung-Hwa Kim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Sensors</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2022Multi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2022Multi.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2022Multi" class="col-sm-8"> <div class="title">Multi-view Multi-modal Head-Gaze Estimation for Advanced Indoor User Interaction</div> <div class="author"> Jung-Hwa Kim and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Computers, Materials &amp; Continua</em>, Jun 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Njoku2022Deep" class="col-sm-8"> <div class="title">Deep Learning Based Data Fusion Methods for Multimodal Emotion Recognition</div> <div class="author"> Judith Nkechinyere Njokuw, Angela C. Caliwag, Wansu Lim, Sangho Kim, Han-Jeong Hwang, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>The Journal of Korean Institute of Communications and Information Sciences</em>, Jan 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2022Kickboard" class="col-sm-8"> <div class="title">Deep Learning-based Electric Kickboard Helmet Detection in Night Driving Environment</div> <div class="author"> Daun Kim and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>The Transactions of the Korean Institute of Electrical Engineers</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Hong2022Implementation" class="col-sm-8"> <div class="title">Implementation of Book Damage Detection Technique Using Deformable DETR</div> <div class="author"> Sumin Hong, Mihwan Yu, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 한국통신학회 2022 동계</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Outstanding Paper</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstanding Paper</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Yu2022Interactive" class="col-sm-8"> <div class="title">Interactive PIPO Painting Design Web Application Using Multimodal Style Transfer</div> <div class="author"> Mihwan Yu and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 대한전자공학회 2022 하계</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Oh2022Food" class="col-sm-8"> <div class="title">Food Expiration Date Estimation System Based on Vision Transformer and EasyOCR</div> <div class="author"> Jiyeon Oh and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In KCC 2022</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button">Outstanding Paper</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Outstanding Paper</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Oroceo2022Edge" class="col-sm-8"> <div class="title">Edge Device-based Emotion Recognition Keyword Extraction System</div> <div class="author"> Paul Angelo Oroceo, Jungin Kim, Yucheol Jeong, <em class="author-self">Jin-Woo Jeong</em>, Sangho Kim, and Wansu Lim </div> <div class="periodical"> <em>In 대한인간공학회 2022 추계</em>, Oct 2022 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2021Web" class="col-sm-8"> <div class="title">Web-based Deep Learning Support System for Object Detection Data Training and Management</div> <div class="author"> JuYeop Kim, Min Young Kim, Ji-Yong Jeon, Eunhye Jeong, Sodam Hyeon, <em class="author-self">Jin-Woo Jeong</em>, and Tae-Hyung Kim </div> <div class="periodical"> <em>In SWCC 2021 하계</em>, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2021Knock.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2021Knock.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2021Knock" class="col-sm-8"> <div class="title">Knock &amp;Tap: Classification and Localization of Knock and Tap Gestures using Deep Sound Transfer Learning</div> <div class="author"> Jae-Yeop Jeong, Jung-Hwa Kim, Ha-Yeong Yoon, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM International Conference on Multi-modal Interaction (ICMI)</em>, Montreal, Canada, Oct 2021 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3461615.3485428" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kwak2021Deep.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kwak2021Deep.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kwak2021Deep" class="col-sm-8"> <div class="title">Deep Learning-based Safety-Assistant System for Micro-mobility Vehicles: Preliminary Study</div> <div class="author"> Hyeon-Seo Kwak, Min-Young Kim, Ji-Yong Jeon, Eun-Hye Jeong, Ju-Yeop Kim, So-Dam Hyeon, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In International Conference on Information Networking (ICOIN)</em>, Jeju, Jan 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Hong2021Proposal" class="col-sm-8"> <div class="title">Proposal of a Deep Learning-based Used Book Damage Detection Technique</div> <div class="author"> Sumin Hong, Jung-Hwa Kim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 대한인간공학회 2021 추계</em>, Jan 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2021Deep" class="col-sm-8"> <div class="title">Deep Learning-based Effector Classification System for Electric Guitar Players</div> <div class="author"> Namsub Kim, Jae-Yeop Jeong, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 대한인간공학회 2021 추계</em>, Jan 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2021Proposal" class="col-sm-8"> <div class="title">Proposal of Instructor’s Gaze Visualization System for Enhancing Learner’s Learning Efficiency in Online Education</div> <div class="author"> Daun Kim, Namsub Kim, Sumin Hong, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 한국통신학회 2021 추계</em>, Jan 2021 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Ha2020Temporal.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Ha2020Temporal.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ha2020Temporal" class="col-sm-8"> <div class="title">Temporal Pyramid Pooling for Decoding Motor-Imagery EEG Signals</div> <div class="author"> Kwon-Woo Ha and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>IEEE Access</em>, Jan 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2020Gaze.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2020Gaze.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2020Gaze" class="col-sm-8"> <div class="title">Gaze in the Dark: Gaze Estimation in a Low-light Environment with Generative Adversarial Networks</div> <div class="author"> Jung-Hwa Kim and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Sensors</em>, Jan 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2020GazeDark.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2020GazeDark.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2020GazeDark" class="col-sm-8"> <div class="title">Gaze Estimation in the Dark with Generative Adversarial Networks</div> <div class="author"> Jung-Hwa Kim and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM ETRA Workshop on Eye Tracking for Quality of Experience in Multimedia (ET-MM’20)</em>, Germany, Jun 2020 </div> <div class="periodical"> </div> <div class="links"> <a href="https://doi.org/10.1145/3517031.3529238" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Top-Tier Conference</abbr> <figure> <picture> <img src="/assets/img/publication_preview/Kim2020Preliminary.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2020Preliminary.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2020Preliminary" class="col-sm-8"> <div class="title">A Preliminary Study on Performance Evaluation of Multi-View Multi-Modal Gaze Estimation under Challenging Conditions</div> <div class="author"> Jung-Hwa Kim and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In ACM CHI Conference on Human Factors in Computing Systems (CHI’20) Late-Breaking Works</em>, USA, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jo2020Prediction.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jo2020Prediction.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jo2020Prediction" class="col-sm-8"> <div class="title">Prediction of Visual Memorability with EEG Signals using Deep Neural Networks</div> <div class="author"> Sang-Yeong Jo and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In The 8th IEEE International Conference on Brain Computer Interface</em>, Korea, Feb 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jo2020Prediction.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jo2020Prediction.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jo2020Predictioo" class="col-sm-8"> <div class="title">Prediction of Visual Memorability with EEG Signals: A Comparative Study</div> <div class="author"> Sang-Yeong Jo and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Sensors</em>, Feb 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Jang2020Smart" class="col-sm-8"> <div class="title">Design and Implementation of Smart Crosswalk System through Deep Learning-based Vehicle Recognition and Speed Estimation on Edge Devices</div> <div class="author"> Sun-Hye Jang, Hee-Eun Cho, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Journal of the Korea Institute of Information and Communication Engineering</em>, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2020Cultural" class="col-sm-8"> <div class="title">Analysis of Cultural Characteristics of Image Retrieval using Deep Transfer Learning</div> <div class="author"> Hyeon-Sik Kim and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Journal of the Korea Institute of Information and Communication Engineering</em>, Apr 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2020Stage" class="col-sm-8"> <div class="title">A Study on Stage Tour Content Generation Method based on Deep Learning Style Transfer</div> <div class="author"> Dong-Min Kim, Hyeon-Sik Kim, Dae-Hyun Bong, Jong-Youn Choi, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Journal of the Korea Institute of Information and Communication Engineering</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Ji2020Emotion" class="col-sm-8"> <div class="title">Emotion Classification Based on EEG and Gaze Data using CapsuleNet</div> <div class="author"> Kyu-Bin Ji and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In HCI Korea 2020</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Yoon2020Deep" class="col-sm-8"> <div class="title">Deep Learning-based Sidewalk Condition Recognition for Wheelchair Users</div> <div class="author"> Ha-Yeong Yoon, Jung-Hwa Kim, Hyeon Seo Kwak, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In KSC 2020</em>, Nov 2020 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Ha2019Decoding.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Ha2019Decoding.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ha2019Decoding" class="col-sm-8"> <div class="title">Decoding Two-Class Motor Imagery EEG with Capsule Networks</div> <div class="author"> Kwon-Woo Ha and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In The 6th IEEE International Conference on Big Data and Smart Computing (BigComp 2019)</em>, Kyoto, Japan, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jo2019Scenery.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jo2019Scenery.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jo2019Scenery" class="col-sm-8"> <div class="title">Scenery-based Fashion Recommendation with Cross-Domain Generative Adversarial Networks</div> <div class="author"> Sang-Young Jo, Sun-Hye Jang, Hee-Eun Cho, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In The 6th IEEE International Conference on Big Data and Smart Computing (BigComp 2019)</em>, Kyoto, Japan, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Ha2019Motor.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Ha2019Motor.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ha2019Motor" class="col-sm-8"> <div class="title">Motor Imagery EEG Classification Using Capsule Networks</div> <div class="author"> Kwon-Woo Ha and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Sensors</em>, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2019Watch.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2019Watch.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2019Watch" class="col-sm-8"> <div class="title">Watch &amp; Do: A Smart IoT Interaction System with Object Detection and Gaze Estimation</div> <div class="author"> Jung-Hwa Kim, Seung-June Choi, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>IEEE Transactions on Consumer Electronics</em>, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Hwang2019VisKit.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Hwang2019VisKit.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Hwang2019VisKit" class="col-sm-8"> <div class="title">VisKit: Web-Based Interactive IoT Management with Deep Visual Object Detection</div> <div class="author"> Chae-Eun Hwang, Sung-Hun Lee, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Journal of Sensor and Actuator Networks</em>, Jan 2019 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2018</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Ha2018Decoding.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Ha2018Decoding.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Ha2018Decoding" class="col-sm-8"> <div class="title">Decoding EEG with Capsules: A Preliminary Study</div> <div class="author"> Kwon-Woo Ha and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In The 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC 2018)</em>, Honolulu, HI, Jul 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lee2018Nools" class="col-sm-8"> <div class="title">Design and Implementation of Nools-based Rule Engine for Intelligent IoE Platform</div> <div class="author"> Sung-Hun Lee, Chae-Eun Hwang, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Journal of the Korea Institute of Information, Electronics, and Communication Technology</em>, Aug 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Cho2018Hospital" class="col-sm-8"> <div class="title">Design and Implementation of Hospital Room Management System based on IoT Carebot</div> <div class="author"> Sang-Yeong Jo and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Journal of the Korea Institute of Information, Electronics, and Communication Technology</em>, Aug 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Kim2018IoT" class="col-sm-8"> <div class="title">IoT Control through Deep Learning-based Real-time Object Detection and Gaze Tracking</div> <div class="author"> Jung-Hwa Kim, Seung-June Choi, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 한국정보기술학회 2018 하계</em>, Aug 2018 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Baek2017Thing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Baek2017Thing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Baek2017Thing" class="col-sm-8"> <div class="title">Thing Control System based on IoT Tags and Social Messages</div> <div class="author"> Seung-Min Baek, Yeon-Ju Jin, Kwon-Woo Ha, Sang-Wook Han, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>KIISE Transactions on Computing Practices</em>, Sep 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2017Gesture.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2017Gesture.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2017Gesture" class="col-sm-8"> <div class="title">Design and Implementation of Gesture-based IoT Control System using Wearable Devices</div> <div class="author"> Jun-Su Kim, Seung-Min Baek, Yeon-Ju Jin, Sang-Wook Han, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Journal of Korean Institute of Information Technology</em>, Oct 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Kim2017Crowdsourcing.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Kim2017Crowdsourcing.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Kim2017Crowdsourcing" class="col-sm-8"> <div class="title">Design and Implementation of Crowdsourcing-based Q&amp;A System using Social Chatbot</div> <div class="author"> Bo-Kyung Kim, Hyun-Ah Kim, Chae-Eun Sim, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>Journal of Korean Institute of Information Technology</em>, Nov 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Baek2017Efficient" class="col-sm-8"> <div class="title">Efficient IoT Object Control using Gestures</div> <div class="author"> Seung-Min Baek, Yeon-Ju Jin, Kwon-Woo Ha, Sang-Wook Han, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In 한국정보기술학회 2017 하계</em>, Nov 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Lee2017VisKit" class="col-sm-8"> <div class="title">VisKit: Design of a Video-based Intelligent IoT Control System</div> <div class="author"> Sung-Hun Lee, Kwon-Woo Ha, Chae-Eun Hwang, and <em class="author-self">Jin-Woo Jeong</em> </div> <div class="periodical"> <em>In KSC 2017</em>, Nov 2017 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2014</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2014Automatic.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2014Automatic.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2014Automatic" class="col-sm-8"> <div class="title">Automatic image annotation using affective vocabularies: attribute-based learning approach</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em> and Dong-Ho Lee </div> <div class="periodical"> <em>Journal of Information Science</em>, Nov 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2014Inferring.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2014Inferring.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2014Inferring" class="col-sm-8"> <div class="title">Inferring Search Intents from Remote Control Movement Patterns: A New Content Search Method for Smart TV</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em> and Dong-Ho Lee </div> <div class="periodical"> <em>IEEE Transactions on Consumer Electronics</em>, Nov 2014 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2013</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2013Emotion.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2013Emotion.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2013Emotion" class="col-sm-8"> <div class="title">Emotion-based Image Retrieval and Annotation Method using Attribute Information of Objects</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em> and Dong-Ho Lee </div> <div class="periodical"> <em>Database Research</em>, Apr 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Qasim2013ConceptMap" class="col-sm-8"> <div class="title">Design and Implementation of Concept Map Construction System using Message Passing Technique</div> <div class="author"> Iqbal Qasim, <em class="author-self">Jin-Woo Jeong</em>, Ji-Woog Heo, and Dong-Ho Lee </div> <div class="periodical"> <em>Database Research</em>, Apr 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="Qasim2013Concept" class="col-sm-8"> <div class="title">Concept map construction from text documents using affinity propagation</div> <div class="author"> Qasim, <em class="author-self">Jin-Woo Jeong</em>, Jee-Uk Heu, and Dong-Ho Lee </div> <div class="periodical"> <em>Journal of Information Science</em>, Apr 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2013i.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2013i.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2013i" class="col-sm-8"> <div class="title">i-TagRanker: an efficient tag ranking system for image sharing and retrieval using the semantic relationships between tags</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em>, Hyun-Ki Hong, and Dong-Ho Lee </div> <div class="periodical"> <em>Multimedia Tools and Applications</em>, Apr 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2013Crowd.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2013Crowd.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2013Crowd" class="col-sm-8"> <div class="title">A Crowd-Powered Socially Embedded Search Engine</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em>, Meredith Morris, Jaime Teevan, and Daniel Liebling </div> <div class="periodical"> <em>In The Seventh International AAAI Conference on Weblogs and Social Media (ICWSM 2013)</em>, Boston, MA, Jul 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2013Understanding.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2013Understanding.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2013Understanding" class="col-sm-8"> <div class="title">Understanding the Potential of Social Questions in the Web Search</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em>, Jee-Uk Heu, and Dong-Ho Lee </div> <div class="periodical"> <em>In ACM International Conference on Computer Supported Cooperative Work and Social Computing (CSCW)</em>, USA, Feb 2013 </div> <div class="periodical"> Top 10 of Human Computer Interaction at Google Scholar Metrics </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Heu2013Multi.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Heu2013Multi.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Heu2013Multi" class="col-sm-8"> <div class="title">Multi-Document Summarization Exploiting Semantic Analysis based on Tag Cluster</div> <div class="author"> Jee-Uk Heu, <em class="author-self">Jin-Woo Jeong</em>, Iqbal Qasim, Young-Do Joo, Joon-Myun Cho, and Dong-Ho Lee </div> <div class="periodical"> <em>In 19th International Conference on Multimedia Modeling (MMM)</em>, China, Jan 2013 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2012</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2012Towards.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2012Towards.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2012Towards" class="col-sm-8"> <div class="title">Towards Measuring the Visualness of a Concept</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em>, Xin-Jing Wang, and Dong-Ho Lee </div> <div class="periodical"> <em>In ACM International Conference on Information and Knowledge Management (CIKM)</em>, USA, Oct 2012 </div> <div class="periodical"> Top 10 of Data Mining at Microsoft Academic Ranking </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2012Visual.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2012Visual.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2012Visual" class="col-sm-8"> <div class="title">Visual Summarization of the Social Image Collection using Image Attractiveness Learned from Social Behaviours</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em>, Hyun-Ki Hong, Jee-Uk Heu, Iqbal Qasim, and Dong-Ho Lee </div> <div class="periodical"> <em>In IEEE International Conference on Multimedia &amp; Expo (ICME)</em>, Australia, Jul 2012 </div> <div class="periodical"> Top 10 of Multimedia at Microsoft Academic Ranking </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2011</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2011Exploiting.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2011Exploiting.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2011Exploiting" class="col-sm-8"> <div class="title">Exploiting of Flickr Note and Its Applications for Social Image Sharing and Search</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em>, Hyun-Ki Hong, and Dong-Ho Lee </div> <div class="periodical"> <em>In IEEE International Symposium on Multimedia (ISM)</em>, USA, Dec 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Qasim2011Exploiting.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Qasim2011Exploiting.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Qasim2011Exploiting" class="col-sm-8"> <div class="title">Exploiting Affinity Propagation for Automatic Acquisition of Domain Concept in Ontology Learning</div> <div class="author"> Iqbal Qasim, <em class="author-self">Jin-Woo Jeong</em>, Sharifullah Khan, and Dong-Ho Lee </div> <div class="periodical"> <em>In 7th IEEE International Conference on Emerging Technologies (ICET)</em>, Pakistan, Sep 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2011Ontology.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2011Ontology.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2011Ontology" class="col-sm-8"> <div class="title">Ontology-based Automatic Video Annotation Technique in Smart TV Environment</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em>, Hyun-Ki Hong, and Dong-Ho Lee </div> <div class="periodical"> <em>In IEEE Transactions on Consumer Electronics</em>, Sep 2011 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography">2010</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Hong2010Efficient.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Hong2010Efficient.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Hong2010Efficient" class="col-sm-8"> <div class="title">Efficient Tag Ranking Method based on Semantic Relationships between Image Tags</div> <div class="author"> Hyun-Ki Hong, <em class="author-self">Jin-Woo Jeong</em>, and Dong-Ho Lee </div> <div class="periodical"> <em>Database Research</em>, Dec 2010 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li></ol> <h2 class="bibliography">2007</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Park2007OLYBIA.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Park2007OLYBIA.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Park2007OLYBIA" class="col-sm-8"> <div class="title">OLYBIA : Ontology-based Automatic Image Annotation System using Semantic Inference Rules</div> <div class="author"> Kyung-Wook Park, <em class="author-self">Jin-Woo Jeong</em>, and Dong-Ho Lee </div> <div class="periodical"> <em>In Lecture Note in Computer Science (LNCS)</em>, Dec 2007 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <figure> <picture> <img src="/assets/img/publication_preview/Jeong2007Automatic.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="Jeong2007Automatic.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Jeong2007Automatic" class="col-sm-8"> <div class="title">Automatic Extraction of Semantic Relationships from Images Using Ontologies and SVM Classifiers</div> <div class="author"> <em class="author-self">Jin-Woo Jeong</em>, Kyung-Wook Park, OukSeh Lee, and Dong-Ho Lee </div> <div class="periodical"> <em>In Lecture Notes in Computer Science(LNCS)</em>, Dec 2007 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> </ol> <h2 class="bibliography"></h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TBD2024Enhancing" class="col-sm-8"> <div class="title">Enhancing Multitasking in Mixed Reality: Design and Assessment of Visual Aids for Managing Interruptions</div> <div class="author"> TBD </div> <div class="periodical"> Dec 2007 </div> <div class="periodical"> Under review </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TBD2023MuNext" class="col-sm-8"> <div class="title">MuNext: Multi-task UNeXt for Efficient Breast Cancer Classification and Segmentation</div> <div class="author"> TBD </div> <div class="periodical"> Dec 2007 </div> <div class="periodical"> Under review </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TBD2023Classification" class="col-sm-8"> <div class="title">Classification of Breast Cancer from Mammograms: Do Vision Transformers and MLP-Mixers Outperform CNNs?</div> <div class="author"> TBD </div> <div class="periodical"> Dec 2007 </div> <div class="periodical"> Under review </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> </div> <div id="TBD2023Efficient" class="col-sm-8"> <div class="title">Efficient Multi-Task Learning for Facial Expression Recognition In-the-wild</div> <div class="author"> TBD </div> <div class="periodical"> Dec 2007 </div> <div class="periodical"> Under review </div> <div class="links"> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Interaction Lab.. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>